{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d93044cf",
   "metadata": {},
   "source": [
    "# Image_ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e26f66",
   "metadata": {},
   "source": [
    "## Open CV를 이용한 MNIST 데이터 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7681be49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape= (60000, 784)\n",
      "y_train.shape= (60000, 10)\n",
      "x_test.shape= (10000, 784)\n",
      "y_test.shape= (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "''' ref1: http://yann.lecun.com/exdb/mnist/\n",
    "    ref2: https://gist.github.com/ischlag/41d15424e7989b936c1609b53edd1390\n",
    "'''\n",
    "import gzip\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "IMAGE_SIZE = 28\n",
    "PIXEL_DEPTH = 255\n",
    "NUM_LABELS = 10\n",
    "\n",
    "#1\n",
    "def extract_data(filename, num_images):\n",
    "  '''Extract the images into a 4D tensor [image index, y, x, channels].\n",
    "     Values are rescaled from [0, 255] down to [0, 1].\n",
    "  '''\n",
    "##  print('Extracting', filename)\n",
    "  with gzip.open(filename) as bytestream:\n",
    "    bytestream.read(16)\n",
    "    buf = bytestream.read(IMAGE_SIZE * IMAGE_SIZE * num_images)\n",
    "    data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
    "##    data = data/PIXEL_DEPTH\n",
    "    data = data.reshape(num_images, IMAGE_SIZE, IMAGE_SIZE)\n",
    "    return data\n",
    "\n",
    "def extract_labels(filename, num_images):\n",
    "  '''Extract the labels into a vector of int64 label IDs.'''\n",
    "##  print('Extracting', filename)\n",
    "  with gzip.open(filename) as bytestream:\n",
    "    bytestream.read(8)\n",
    "    buf = bytestream.read(1 * num_images)\n",
    "    labels = np.frombuffer(buf, dtype=np.uint8).astype(np.int32)\n",
    "  return labels\n",
    "\n",
    "def ont_hot_encoding(y): # assume that y is 1-D array\n",
    "    t = np.zeros((y.size, 10), dtype=np.float32)\n",
    "    for i, row in enumerate(t):\n",
    "        row[y[i]] = 1      \n",
    "    return t\n",
    "  \n",
    "# Extract it into np arrays.\n",
    "def load_MINIST(flatten=True, one_hot=True):\n",
    "  x_train=extract_data('./data/train-images-idx3-ubyte.gz',  60000)\n",
    "  y_train=extract_labels('./data/train-labels-idx1-ubyte.gz',60000)\n",
    "  x_test =extract_data('./data/t10k-images-idx3-ubyte.gz',   10000)\n",
    "  y_test =extract_labels('./data/t10k-labels-idx1-ubyte.gz', 10000)\n",
    "\n",
    "  if flatten:\n",
    "    x_train= x_train.reshape(-1, IMAGE_SIZE*IMAGE_SIZE) # (60000, 784)\n",
    "    x_test = x_test.reshape(-1, IMAGE_SIZE*IMAGE_SIZE)  # (10000, 784)\n",
    "  if one_hot:\n",
    "    y_train = ont_hot_encoding(y_train)\n",
    "    y_test = ont_hot_encoding(y_test)    \n",
    "  return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "#2\n",
    "(x_train, y_train), (x_test, y_test) = load_MINIST()\n",
    "print('x_train.shape=', x_train.shape) # (60000, 784)\n",
    "print('y_train.shape=', y_train.shape) # (60000, 10)\n",
    "print('x_test.shape=',  x_test.shape)  # (10000, 784)\n",
    "print('y_test.shape=',  y_test.shape)  # (10000, 10)\n",
    "\n",
    "dst = np.zeros((20*IMAGE_SIZE, 20*IMAGE_SIZE), dtype=np.uint8)\n",
    "for i in range(400):\n",
    "  x = i%20\n",
    "  y = i//20\n",
    "  x1 = x*IMAGE_SIZE\n",
    "  y1 = y*IMAGE_SIZE\n",
    "  x2 = x1+IMAGE_SIZE\n",
    "  y2 = y1+IMAGE_SIZE  \n",
    "  \n",
    "  img = x_train[i].astype(np.uint8)\n",
    "  img = img.reshape(IMAGE_SIZE, IMAGE_SIZE)\n",
    "  dst[y1:y2, x1:x2] = img\n",
    "\n",
    "cv2.imshow('MINIST 400', dst)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a78bbcd",
   "metadata": {},
   "source": [
    "## OpenCV 의 인공 신경망 분류 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211f82ef",
   "metadata": {},
   "source": [
    "### 2층 신경망 랜덤 샘플링 미니배치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b25797c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_accuracy[0]=0.8647166666666667, train_loss=2.137570833333333\n",
      "train_accuracy[60]=0.88525, train_loss=2.0584998697916665\n",
      "train_accuracy[120]=0.89495, train_loss=2.0082640625\n",
      "train_accuracy[180]=0.8987833333333334, train_loss=1.984890625\n",
      "train_accuracy[240]=0.8917, train_loss=2.0098609375\n",
      "train_accuracy[300]=0.90835, train_loss=1.960669921875\n",
      "train_accuracy[360]=0.9103333333333333, train_loss=1.946646875\n",
      "train_accuracy[420]=0.9121166666666667, train_loss=1.9405240885416666\n",
      "train_accuracy[480]=0.9163666666666667, train_loss=1.9270641927083334\n",
      "train_accuracy[540]=0.9134333333333333, train_loss=1.933655859375\n",
      "train_accuracy[600]=0.9160333333333334, train_loss=1.9169928385416666\n",
      "train_accuracy[660]=0.9202, train_loss=1.9119923177083333\n",
      "train_accuracy[720]=0.9203166666666667, train_loss=1.9059169270833334\n",
      "train_accuracy[780]=0.9204666666666667, train_loss=1.8953319010416667\n",
      "train_accuracy[840]=0.92145, train_loss=1.9087641927083334\n",
      "train_accuracy[900]=0.925, train_loss=1.9014984375\n",
      "train_accuracy[960]=0.9245166666666667, train_loss=1.89378984375\n",
      "train_accuracy[1020]=0.9268333333333333, train_loss=1.88447890625\n",
      "train_accuracy[1080]=0.9296333333333333, train_loss=1.8696545572916667\n",
      "train_accuracy[1140]=0.9298333333333333, train_loss=1.8734881510416668\n",
      "train_accuracy[1200]=0.931, train_loss=1.8664963541666666\n",
      "train_accuracy[1260]=0.9272333333333334, train_loss=1.8799045572916666\n",
      "train_accuracy[1320]=0.9276333333333333, train_loss=1.8757444010416666\n",
      "train_accuracy[1380]=0.929, train_loss=1.8746009114583333\n",
      "train_accuracy[1440]=0.93125, train_loss=1.862258984375\n",
      "train_accuracy[1500]=0.9326, train_loss=1.8575533854166666\n",
      "train_accuracy[1560]=0.9317666666666666, train_loss=1.8632334635416667\n",
      "train_accuracy[1620]=0.9306666666666666, train_loss=1.871043359375\n",
      "train_accuracy[1680]=0.9344333333333333, train_loss=1.8531873697916668\n",
      "train_accuracy[1740]=0.9367, train_loss=1.8532395833333333\n",
      "train_accuracy[1800]=0.9360333333333334, train_loss=1.8467395833333333\n",
      "train_accuracy[1860]=0.9359333333333333, train_loss=1.848830859375\n",
      "train_accuracy[1920]=0.9357166666666666, train_loss=1.85173984375\n",
      "train_accuracy[1980]=0.9355166666666667, train_loss=1.85153828125\n",
      "train_accuracy[2040]=0.93755, train_loss=1.8441680989583333\n",
      "train_accuracy[2100]=0.9362, train_loss=1.848865234375\n",
      "train_accuracy[2160]=0.93835, train_loss=1.8374826822916666\n",
      "train_accuracy[2220]=0.93815, train_loss=1.8408858072916667\n",
      "train_accuracy[2280]=0.9391333333333334, train_loss=1.8411869791666666\n",
      "train_accuracy[2340]=0.9395333333333333, train_loss=1.8385359375\n",
      "train_accuracy[2400]=0.9369333333333333, train_loss=1.842088671875\n",
      "train_accuracy[2460]=0.9392, train_loss=1.8369321614583334\n",
      "train_accuracy[2520]=0.93565, train_loss=1.8565263020833334\n",
      "train_accuracy[2580]=0.9376833333333333, train_loss=1.8444973958333333\n",
      "train_accuracy[2640]=0.9384333333333333, train_loss=1.8380639322916668\n",
      "train_accuracy[2700]=0.93885, train_loss=1.829406640625\n",
      "train_accuracy[2760]=0.9385, train_loss=1.8351729166666666\n",
      "train_accuracy[2820]=0.9377666666666666, train_loss=1.8400756510416667\n",
      "train_accuracy[2880]=0.9392166666666667, train_loss=1.8326627604166668\n",
      "train_accuracy[2940]=0.93815, train_loss=1.846187109375\n",
      "train_accuracy[3000]=0.9386333333333333, train_loss=1.8394174479166667\n",
      "train_accuracy[3060]=0.9375666666666667, train_loss=1.8421186197916666\n",
      "train_accuracy[3120]=0.9382166666666667, train_loss=1.8480528645833334\n",
      "train_accuracy[3180]=0.9395666666666667, train_loss=nan\n",
      "train_accuracy[3240]=0.9389666666666666, train_loss=1.83751171875\n",
      "train_accuracy[3300]=0.9412833333333334, train_loss=1.832453515625\n",
      "train_accuracy[3360]=0.9399166666666666, train_loss=1.836443359375\n",
      "train_accuracy[3420]=0.9414166666666667, train_loss=nan\n",
      "train_accuracy[3480]=0.94005, train_loss=1.8409265625\n",
      "train_accuracy[3540]=0.9404, train_loss=1.830102734375\n",
      "train_accuracy[3600]=0.9421666666666667, train_loss=1.8282119791666667\n",
      "train_accuracy[3660]=0.9401166666666667, train_loss=1.8358615885416667\n",
      "train_accuracy[3720]=0.9411333333333334, train_loss=1.8288135416666667\n",
      "train_accuracy[3780]=0.9415333333333333, train_loss=1.8252571614583333\n",
      "train_accuracy[3840]=0.9433166666666667, train_loss=1.82029453125\n",
      "train_accuracy[3900]=0.94345, train_loss=1.8188811197916666\n",
      "train_accuracy[3960]=0.9432666666666667, train_loss=1.8253977864583333\n",
      "train_accuracy[4020]=0.9405833333333333, train_loss=1.8317462239583333\n",
      "train_accuracy[4080]=0.9410833333333334, train_loss=1.8364533854166667\n",
      "train_accuracy[4140]=0.94405, train_loss=1.822346484375\n",
      "train_accuracy[4200]=0.94435, train_loss=1.818952734375\n",
      "train_accuracy[4260]=0.9417166666666666, train_loss=1.824634375\n",
      "train_accuracy[4320]=0.9425666666666667, train_loss=nan\n",
      "train_accuracy[4380]=0.9440166666666666, train_loss=1.82103984375\n",
      "train_accuracy[4440]=0.9432333333333334, train_loss=1.819216796875\n",
      "train_accuracy[4500]=0.9443, train_loss=1.8181852864583334\n",
      "train_accuracy[4560]=0.9432166666666667, train_loss=1.8147188802083334\n",
      "train_accuracy[4620]=0.942, train_loss=1.8220645833333333\n",
      "train_accuracy[4680]=0.9419666666666666, train_loss=1.8237298177083334\n",
      "train_accuracy[4740]=0.9442833333333334, train_loss=1.8224075520833334\n",
      "train_accuracy[4800]=0.9456333333333333, train_loss=1.8092733072916667\n",
      "train_accuracy[4860]=0.94335, train_loss=1.8226104166666666\n",
      "train_accuracy[4920]=0.9455, train_loss=1.8185903645833332\n",
      "train_accuracy[4980]=0.9456333333333333, train_loss=1.8089123697916667\n",
      "train_accuracy[5040]=0.9444833333333333, train_loss=1.8135048177083333\n",
      "train_accuracy[5100]=0.94415, train_loss=1.816919140625\n",
      "train_accuracy[5160]=0.9439333333333333, train_loss=1.817370703125\n",
      "train_accuracy[5220]=0.9424, train_loss=1.82109609375\n",
      "train_accuracy[5280]=0.9456333333333333, train_loss=1.8098833333333333\n",
      "train_accuracy[5340]=0.94595, train_loss=1.8134484375\n",
      "train_accuracy[5400]=0.9466333333333333, train_loss=1.817627734375\n",
      "train_accuracy[5460]=0.9449666666666666, train_loss=1.8132494791666667\n",
      "train_accuracy[5520]=0.9442666666666667, train_loss=1.8102475260416666\n",
      "train_accuracy[5580]=0.9437666666666666, train_loss=1.8175869791666666\n",
      "train_accuracy[5640]=0.9470666666666666, train_loss=1.8090028645833334\n",
      "train_accuracy[5700]=0.94495, train_loss=1.8137143229166666\n",
      "train_accuracy[5760]=0.9446833333333333, train_loss=1.8163283854166667\n",
      "train_accuracy[5820]=0.9469333333333333, train_loss=nan\n",
      "train_accuracy[5880]=0.947, train_loss=1.8118869791666667\n",
      "train_accuracy[5940]=0.9470333333333333, train_loss=1.8066190104166666\n",
      "train_accuracy[6000]=0.9471666666666667, train_loss=1.814500390625\n",
      "train_accuracy[6060]=0.9461, train_loss=1.8162772135416667\n",
      "train_accuracy[6120]=0.94445, train_loss=1.817743359375\n",
      "train_accuracy[6180]=0.9462833333333334, train_loss=1.8128833333333334\n",
      "train_accuracy[6240]=0.9455166666666667, train_loss=1.8121298177083334\n",
      "train_accuracy[6300]=0.9476333333333333, train_loss=1.80866171875\n",
      "train_accuracy[6360]=0.9462166666666667, train_loss=1.8105108072916667\n",
      "train_accuracy[6420]=0.94565, train_loss=1.8161708333333333\n",
      "train_accuracy[6480]=0.9468666666666666, train_loss=1.809027734375\n",
      "train_accuracy[6540]=0.94735, train_loss=1.8076772135416668\n",
      "train_accuracy[6600]=0.9450666666666667, train_loss=1.8129598958333333\n",
      "train_accuracy[6660]=0.94665, train_loss=1.8080852864583334\n",
      "train_accuracy[6720]=0.9457, train_loss=1.8164497395833332\n",
      "train_accuracy[6780]=0.9472, train_loss=1.809657421875\n",
      "train_accuracy[6840]=0.9450833333333334, train_loss=nan\n",
      "train_accuracy[6900]=0.9456166666666667, train_loss=1.81310234375\n",
      "train_accuracy[6960]=0.9469833333333333, train_loss=1.8079938802083333\n",
      "train_accuracy[7020]=0.94745, train_loss=1.80917734375\n",
      "train_accuracy[7080]=0.9470666666666666, train_loss=1.8082071614583333\n",
      "train_accuracy[7140]=0.9471166666666667, train_loss=1.8056859375\n",
      "train_accuracy[7200]=0.9480833333333333, train_loss=1.80355078125\n",
      "train_accuracy[7260]=0.9483666666666667, train_loss=nan\n",
      "train_accuracy[7320]=0.94745, train_loss=1.8114765625\n",
      "train_accuracy[7380]=0.9470666666666666, train_loss=nan\n",
      "train_accuracy[7440]=0.94715, train_loss=1.8024661458333333\n",
      "train_accuracy[7500]=0.94905, train_loss=1.80250625\n",
      "train_accuracy[7560]=0.9482166666666667, train_loss=1.8012955729166666\n",
      "train_accuracy[7620]=0.9493666666666667, train_loss=1.800674609375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_accuracy[7680]=0.9497166666666667, train_loss=1.79722578125\n",
      "train_accuracy[7740]=0.9468, train_loss=1.8052684895833333\n",
      "train_accuracy[7800]=0.95025, train_loss=1.7978463541666667\n",
      "train_accuracy[7860]=0.9492333333333334, train_loss=1.8023747395833334\n",
      "train_accuracy[7920]=0.9484666666666667, train_loss=1.8010420572916668\n",
      "train_accuracy[7980]=0.9485833333333333, train_loss=1.809854296875\n",
      "train_accuracy[8040]=0.9484666666666667, train_loss=1.805144140625\n",
      "train_accuracy[8100]=0.9466333333333333, train_loss=1.81328125\n",
      "train_accuracy[8160]=0.9483833333333334, train_loss=1.8022708333333333\n",
      "train_accuracy[8220]=0.9475833333333333, train_loss=1.8062204427083333\n",
      "train_accuracy[8280]=0.9505666666666667, train_loss=1.8085927083333333\n",
      "train_accuracy[8340]=0.9487666666666666, train_loss=1.8000412760416666\n",
      "train_accuracy[8400]=0.9487333333333333, train_loss=1.8045203125\n",
      "train_accuracy[8460]=0.9491333333333334, train_loss=1.7997178385416666\n",
      "train_accuracy[8520]=0.9481333333333334, train_loss=1.8032305989583333\n",
      "train_accuracy[8580]=0.9470833333333334, train_loss=1.8043826822916667\n",
      "train_accuracy[8640]=0.9490166666666666, train_loss=1.8022415364583333\n",
      "train_accuracy[8700]=0.9486666666666667, train_loss=1.8053067708333332\n",
      "train_accuracy[8760]=0.9490833333333333, train_loss=1.80328046875\n",
      "train_accuracy[8820]=0.94835, train_loss=1.8013419270833333\n",
      "train_accuracy[8880]=0.9458333333333333, train_loss=nan\n",
      "train_accuracy[8940]=0.94765, train_loss=1.8059709635416668\n",
      "train_accuracy[9000]=0.9487833333333333, train_loss=1.801651171875\n",
      "train_accuracy[9060]=0.9488, train_loss=1.8021735677083333\n",
      "train_accuracy[9120]=0.9492333333333334, train_loss=1.800466015625\n",
      "train_accuracy[9180]=0.9475833333333333, train_loss=1.8025671875\n",
      "train_accuracy[9240]=0.9473, train_loss=1.80424921875\n",
      "train_accuracy[9300]=0.9489833333333333, train_loss=1.8012052083333334\n",
      "train_accuracy[9360]=0.9497333333333333, train_loss=1.7985131510416668\n",
      "train_accuracy[9420]=0.9500166666666666, train_loss=1.7989901041666667\n",
      "train_accuracy[9480]=0.9494166666666667, train_loss=nan\n",
      "train_accuracy[9540]=0.9481833333333334, train_loss=1.8037110677083332\n",
      "train_accuracy[9600]=0.95005, train_loss=1.7984837239583333\n",
      "train_accuracy[9660]=0.9482, train_loss=1.8076774739583332\n",
      "train_accuracy[9720]=0.9493166666666667, train_loss=1.8074416666666666\n",
      "train_accuracy[9780]=0.9494833333333333, train_loss=1.7983578125\n",
      "train_accuracy[9840]=0.9485166666666667, train_loss=1.8011611979166666\n",
      "train_accuracy[9900]=0.9492166666666667, train_loss=nan\n",
      "train_accuracy[9960]=0.9482, train_loss=1.801140625\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "''' ref1: http://yann.lecun.com/exdb/mnist/\n",
    "    ref2: https://gist.github.com/ischlag/41d15424e7989b936c1609b53edd1390\n",
    "'''\n",
    "import gzip\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "IMAGE_SIZE  = 28\n",
    "PIXEL_DEPTH = 255\n",
    "NUM_LABELS  = 10\n",
    "\n",
    "#1\n",
    "def extract_data(filename, num_images):\n",
    "  '''Extract the images into a 4D tensor [image index, y, x, channels].\n",
    "     Values are rescaled from [0, 255] down to [0, 1].\n",
    "  '''\n",
    "##  print('Extracting', filename)\n",
    "  with gzip.open(filename) as bytestream:\n",
    "    bytestream.read(16)\n",
    "    buf = bytestream.read(IMAGE_SIZE * IMAGE_SIZE * num_images)\n",
    "    data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
    "##    data = data/PIXEL_DEPTH\n",
    "    data = data.reshape(num_images, IMAGE_SIZE, IMAGE_SIZE)\n",
    "    return data\n",
    "\n",
    "def extract_labels(filename, num_images):\n",
    "  '''Extract the labels into a vector of int64 label IDs.'''\n",
    "##  print('Extracting', filename)\n",
    "  with gzip.open(filename) as bytestream:\n",
    "    bytestream.read(8)\n",
    "    buf = bytestream.read(1 * num_images)\n",
    "    labels = np.frombuffer(buf, dtype=np.uint8).astype(np.int32)\n",
    "  return labels\n",
    "\n",
    "def ont_hot_encoding(y): # assume that y is 1-D array\n",
    "    t = np.zeros((y.size, 10), dtype=np.float32)\n",
    "    for i, row in enumerate(t):\n",
    "        row[y[i]] = 1      \n",
    "    return t\n",
    "  \n",
    "# Extract it into np arrays.\n",
    "def load_MINIST(flatten=True, one_hot=True):\n",
    "  x_train=extract_data('./data/train-images-idx3-ubyte.gz',  60000)\n",
    "  y_train=extract_labels('./data/train-labels-idx1-ubyte.gz',60000)\n",
    "  x_test =extract_data('./data/t10k-images-idx3-ubyte.gz',   10000)\n",
    "  y_test =extract_labels('./data/t10k-labels-idx1-ubyte.gz', 10000)\n",
    "\n",
    "  if flatten:\n",
    "    x_train= x_train.reshape(-1, IMAGE_SIZE*IMAGE_SIZE) # (60000, 784)\n",
    "    x_test = x_test.reshape(-1, IMAGE_SIZE*IMAGE_SIZE)  # (10000, 784)\n",
    "  if one_hot:\n",
    "    y_train = ont_hot_encoding(y_train)\n",
    "    y_test = ont_hot_encoding(y_test)    \n",
    "  return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = load_MINIST()\n",
    "#2\n",
    "ann = cv2.ml.ANN_MLP_create()\n",
    "ann.setLayerSizes(np.array([784, 100, 10]))\n",
    "ann.setTrainMethod(cv2.ml.ANN_MLP_BACKPROP)\n",
    "ann.setActivationFunction(cv2.ml.ANN_MLP_SIGMOID_SYM)\n",
    "ann.setTermCriteria((cv2.TERM_CRITERIA_EPS+cv2.TERM_CRITERIA_COUNT,1,1e-5))\n",
    "\n",
    "trainData = cv2.ml.TrainData_create(samples=x_train,\n",
    "                                    layout=cv2.ml.ROW_SAMPLE,\n",
    "                                    responses=y_train)\n",
    "ret = ann.train(trainData)\n",
    "\n",
    "#3\n",
    "train_loss_list     = []\n",
    "train_accuracy_list = []\n",
    "test_loss_list      = []\n",
    "test_accuracy_list  = []\n",
    "\n",
    "batch_size = 1000\n",
    "train_size = 60000\n",
    "\n",
    "iters_num  = 10000\n",
    "iter_per_epoch = train_size//batch_size\n",
    "\n",
    "for i in range(iters_num):\n",
    "#3-1\n",
    "  batch_mask = np.random.choice(train_size, batch_size)\n",
    "  x_batch = x_train[batch_mask]\n",
    "  y_batch = y_train[batch_mask]\n",
    "  \n",
    "  trainData = cv2.ml.TrainData_create(samples=x_batch,\n",
    "                                      layout=cv2.ml.ROW_SAMPLE,\n",
    "                                      responses=y_batch)    \n",
    "  ret = ann.train(trainData, flags=cv2.ml.ANN_MLP_UPDATE_WEIGHTS)\n",
    "\n",
    "#3-2\n",
    "  if i % iter_per_epoch == 0:    \n",
    "    y_target = np.argmax(y_train, axis=1)\n",
    "    ret, res_train = ann.predict(x_train)\n",
    "    y_predict = np.argmax(res_train, axis = 1)\n",
    "    train_accuracy = np.sum(y_target==y_predict)/len(y_target)\n",
    "    train_loss = np.sum((y_train-res_train)**2)\n",
    "    train_loss /= x_train.shape[0] # 60000\n",
    "    train_accuracy_list.append(train_accuracy)\n",
    "    train_loss_list.append(train_loss)\n",
    "\n",
    "    y_target = np.argmax(y_test, axis=1)\n",
    "    ret, res_test = ann.predict(x_test)\n",
    "    y_predict = np.argmax(res_test, axis = 1)\n",
    "    test_accuracy = np.sum(y_target==y_predict)/len(y_target)\n",
    "    test_loss = np.sum((y_test-res_test)**2)\n",
    "    test_loss /= x_test.shape[0] # 10000\n",
    "    test_accuracy_list.append(test_accuracy)\n",
    "    test_loss_list.append(test_loss)\n",
    "    \n",
    "    print('train_accuracy[{}]={}, '.format(i, train_accuracy), end='')\n",
    "    print('train_loss={}'.format(train_loss))\n",
    "\n",
    "#4\n",
    "ann.save('./data/ann-minist_2layer_BP.train')\n",
    "\n",
    "x = list(range(len(train_loss_list)))\n",
    "plt.plot(x, train_loss_list, label='train_loss')\n",
    "plt.plot(x, test_loss_list, label='test_loss')\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(x, train_accuracy_list, label='train_accuracy')\n",
    "plt.plot(x, test_accuracy_list, label='test_accuracy')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4317e6",
   "metadata": {},
   "source": [
    "### 미니배치를 이용하지 않고 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87484045",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_accuracy[0]=0.9280333333333334, test_accuracy=0.903\n",
      "train_accuracy[1]=0.9226833333333333, test_accuracy=0.8878\n",
      "train_accuracy[2]=0.9163333333333333, test_accuracy=0.8807\n",
      "train_accuracy[3]=0.9225166666666667, test_accuracy=0.8912\n",
      "train_accuracy[4]=0.9298166666666666, test_accuracy=0.9021\n",
      "train_accuracy[5]=0.9316333333333333, test_accuracy=0.9094\n",
      "train_accuracy[6]=0.9420833333333334, test_accuracy=0.9193\n",
      "train_accuracy[7]=0.94235, test_accuracy=0.9271\n",
      "train_accuracy[8]=0.9459166666666666, test_accuracy=0.9287\n",
      "train_accuracy[9]=0.9449, test_accuracy=0.9266\n",
      "train_accuracy[10]=0.94635, test_accuracy=0.9257\n",
      "train_accuracy[11]=0.94815, test_accuracy=0.9283\n",
      "train_accuracy[12]=0.94775, test_accuracy=0.9266\n",
      "train_accuracy[13]=0.9511833333333334, test_accuracy=0.9292\n",
      "train_accuracy[14]=0.9528, test_accuracy=0.9309\n",
      "train_accuracy[15]=0.95245, test_accuracy=0.9318\n",
      "train_accuracy[16]=0.9544833333333334, test_accuracy=0.9326\n",
      "train_accuracy[17]=0.9533166666666667, test_accuracy=0.9306\n",
      "train_accuracy[18]=0.9524333333333334, test_accuracy=0.9292\n",
      "train_accuracy[19]=0.9547666666666667, test_accuracy=0.9293\n",
      "train_accuracy[20]=0.9529333333333333, test_accuracy=0.9324\n",
      "train_accuracy[21]=0.9540166666666666, test_accuracy=0.929\n",
      "train_accuracy[22]=0.9565166666666667, test_accuracy=0.9334\n",
      "train_accuracy[23]=0.9562, test_accuracy=0.9314\n",
      "train_accuracy[24]=0.9575166666666667, test_accuracy=0.9322\n",
      "train_accuracy[25]=0.95805, test_accuracy=0.9315\n",
      "train_accuracy[26]=0.95865, test_accuracy=0.9301\n",
      "train_accuracy[27]=0.9585166666666667, test_accuracy=0.9323\n",
      "train_accuracy[28]=0.9589, test_accuracy=0.9316\n",
      "train_accuracy[29]=0.9583333333333334, test_accuracy=0.9337\n",
      "train_accuracy[30]=0.9592666666666667, test_accuracy=0.9341\n",
      "train_accuracy[31]=0.9594333333333334, test_accuracy=0.9328\n",
      "train_accuracy[32]=0.9611333333333333, test_accuracy=0.9354\n",
      "train_accuracy[33]=0.9597, test_accuracy=0.9325\n",
      "train_accuracy[34]=0.9606166666666667, test_accuracy=0.9315\n",
      "train_accuracy[35]=0.9601666666666666, test_accuracy=0.9333\n",
      "train_accuracy[36]=0.9584166666666667, test_accuracy=0.9314\n",
      "train_accuracy[37]=0.9607166666666667, test_accuracy=0.9292\n",
      "train_accuracy[38]=0.9580333333333333, test_accuracy=0.9276\n",
      "train_accuracy[39]=0.9606833333333333, test_accuracy=0.9336\n",
      "train_accuracy[40]=0.9613833333333334, test_accuracy=0.9313\n",
      "train_accuracy[41]=0.9599166666666666, test_accuracy=0.9327\n",
      "train_accuracy[42]=0.9604333333333334, test_accuracy=0.9305\n",
      "train_accuracy[43]=0.9606666666666667, test_accuracy=0.9319\n",
      "train_accuracy[44]=0.9610166666666666, test_accuracy=0.9338\n",
      "train_accuracy[45]=0.9614833333333334, test_accuracy=0.9323\n",
      "train_accuracy[46]=0.9620333333333333, test_accuracy=0.9342\n",
      "train_accuracy[47]=0.9622166666666667, test_accuracy=0.9326\n",
      "train_accuracy[48]=0.9624, test_accuracy=0.9326\n",
      "train_accuracy[49]=0.9652166666666666, test_accuracy=0.9351\n",
      "train_accuracy[50]=0.96355, test_accuracy=0.9344\n",
      "train_accuracy[51]=0.9630166666666666, test_accuracy=0.9324\n",
      "train_accuracy[52]=0.9646666666666667, test_accuracy=0.9329\n",
      "train_accuracy[53]=0.96445, test_accuracy=0.9321\n",
      "train_accuracy[54]=0.9633, test_accuracy=0.9315\n",
      "train_accuracy[55]=0.9638166666666667, test_accuracy=0.9331\n",
      "train_accuracy[56]=0.9641666666666666, test_accuracy=0.9304\n",
      "train_accuracy[57]=0.9648, test_accuracy=0.9321\n",
      "train_accuracy[58]=0.96265, test_accuracy=0.9302\n",
      "train_accuracy[59]=0.9634833333333334, test_accuracy=0.9304\n",
      "train_accuracy[60]=0.9649666666666666, test_accuracy=0.9348\n",
      "train_accuracy[61]=0.96615, test_accuracy=0.9309\n",
      "train_accuracy[62]=0.9644333333333334, test_accuracy=0.9327\n",
      "train_accuracy[63]=0.9645166666666667, test_accuracy=0.9276\n",
      "train_accuracy[64]=0.96565, test_accuracy=0.9288\n",
      "train_accuracy[65]=0.96485, test_accuracy=0.93\n",
      "train_accuracy[66]=0.9639666666666666, test_accuracy=0.9291\n",
      "train_accuracy[67]=0.9664833333333334, test_accuracy=0.9304\n",
      "train_accuracy[68]=0.9652333333333334, test_accuracy=0.9277\n",
      "train_accuracy[69]=0.9659, test_accuracy=0.9302\n",
      "train_accuracy[70]=0.9643166666666667, test_accuracy=0.9264\n",
      "train_accuracy[71]=0.9629833333333333, test_accuracy=0.9263\n",
      "train_accuracy[72]=0.9620833333333333, test_accuracy=0.9265\n",
      "train_accuracy[73]=0.9628166666666667, test_accuracy=0.9269\n",
      "train_accuracy[74]=0.9637, test_accuracy=0.9267\n",
      "train_accuracy[75]=0.9646333333333333, test_accuracy=0.931\n",
      "train_accuracy[76]=0.96345, test_accuracy=0.9251\n",
      "train_accuracy[77]=0.9655166666666667, test_accuracy=0.9279\n",
      "train_accuracy[78]=0.9662666666666667, test_accuracy=0.9301\n",
      "train_accuracy[79]=0.96595, test_accuracy=0.9306\n",
      "train_accuracy[80]=0.9653333333333334, test_accuracy=0.9285\n",
      "train_accuracy[81]=0.96605, test_accuracy=0.931\n",
      "train_accuracy[82]=0.96585, test_accuracy=0.929\n",
      "train_accuracy[83]=0.9655833333333333, test_accuracy=0.9279\n",
      "train_accuracy[84]=0.9662166666666666, test_accuracy=0.9275\n",
      "train_accuracy[85]=0.9666833333333333, test_accuracy=0.9303\n",
      "train_accuracy[86]=0.9677166666666667, test_accuracy=0.9301\n",
      "train_accuracy[87]=0.96615, test_accuracy=0.9281\n",
      "train_accuracy[88]=0.96615, test_accuracy=0.9279\n",
      "train_accuracy[89]=0.9666833333333333, test_accuracy=0.9294\n",
      "train_accuracy[90]=0.9652333333333334, test_accuracy=0.9265\n",
      "train_accuracy[91]=0.96685, test_accuracy=0.9261\n",
      "train_accuracy[92]=0.9671, test_accuracy=0.9286\n",
      "train_accuracy[93]=0.9665333333333334, test_accuracy=0.9247\n",
      "train_accuracy[94]=0.96715, test_accuracy=0.9273\n",
      "train_accuracy[95]=0.9648666666666667, test_accuracy=0.9245\n",
      "train_accuracy[96]=0.9638666666666666, test_accuracy=0.9245\n",
      "train_accuracy[97]=0.9646666666666667, test_accuracy=0.9249\n",
      "train_accuracy[98]=0.96695, test_accuracy=0.9262\n"
     ]
    }
   ],
   "source": [
    "''' ref1: http://yann.lecun.com/exdb/mnist/\n",
    "    ref2: https://gist.github.com/ischlag/41d15424e7989b936c1609b53edd1390\n",
    "'''\n",
    "import gzip\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "IMAGE_SIZE  = 28\n",
    "PIXEL_DEPTH = 255\n",
    "NUM_LABELS  = 10\n",
    "\n",
    "#1\n",
    "def extract_data(filename, num_images):\n",
    "  '''Extract the images into a 4D tensor [image index, y, x, channels].\n",
    "     Values are rescaled from [0, 255] down to [0, 1].\n",
    "  '''\n",
    "##  print('Extracting', filename)\n",
    "  with gzip.open(filename) as bytestream:\n",
    "    bytestream.read(16)\n",
    "    buf = bytestream.read(IMAGE_SIZE * IMAGE_SIZE * num_images)\n",
    "    data = np.frombuffer(buf, dtype=np.uint8).astype(np.float32)\n",
    "##    data = data/PIXEL_DEPTH\n",
    "    data = data.reshape(num_images, IMAGE_SIZE, IMAGE_SIZE)\n",
    "    return data\n",
    "\n",
    "def extract_labels(filename, num_images):\n",
    "  '''Extract the labels into a vector of int64 label IDs.'''\n",
    "##  print('Extracting', filename)\n",
    "  with gzip.open(filename) as bytestream:\n",
    "    bytestream.read(8)\n",
    "    buf = bytestream.read(1 * num_images)\n",
    "    labels = np.frombuffer(buf, dtype=np.uint8).astype(np.int32)\n",
    "  return labels\n",
    "\n",
    "def ont_hot_encoding(y): # assume that y is 1-D array\n",
    "    t = np.zeros((y.size, 10), dtype=np.float32)\n",
    "    for i, row in enumerate(t):\n",
    "        row[y[i]] = 1      \n",
    "    return t\n",
    "  \n",
    "# Extract it into np arrays.\n",
    "def load_MINIST(flatten=True, one_hot=True):\n",
    "  x_train=extract_data('./data/train-images-idx3-ubyte.gz',60000)\n",
    "  y_train=extract_labels('./data/train-labels-idx1-ubyte.gz',60000)\n",
    "  x_test=extract_data('./data/t10k-images-idx3-ubyte.gz',10000)\n",
    "  y_test=extract_labels('./data/t10k-labels-idx1-ubyte.gz',10000)\n",
    "\n",
    "  if flatten:\n",
    "    x_train= x_train.reshape(-1, IMAGE_SIZE*IMAGE_SIZE) # (60000, 784)\n",
    "    x_test = x_test.reshape(-1, IMAGE_SIZE*IMAGE_SIZE)  # (10000, 784)\n",
    "  if one_hot:\n",
    "    y_train = ont_hot_encoding(y_train)\n",
    "    y_test = ont_hot_encoding(y_test)    \n",
    "  return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = load_MINIST()\n",
    "\n",
    "#2\n",
    "ann = cv2.ml.ANN_MLP_create()\n",
    "ann.setLayerSizes(np.array([784, 100, 10]))\n",
    "##ann.setLayerSizes(np.array([784, 50, 50, 10]))\n",
    "\n",
    "##ann.setTrainMethod(cv2.ml.ANN_MLP_RPROP)\n",
    "ann.setActivationFunction(cv2.ml.ANN_MLP_SIGMOID_SYM)\n",
    "ann.setTermCriteria((cv2.TERM_CRITERIA_EPS+cv2.TERM_CRITERIA_COUNT,20,1e-5))\n",
    "\n",
    "trainData = cv2.ml.TrainData_create(samples=x_train,\n",
    "                                    layout=cv2.ml.ROW_SAMPLE,\n",
    "                                    responses=y_train)\n",
    "ret = ann.train(trainData)\n",
    "\n",
    "#3\n",
    "train_loss_list     = []\n",
    "train_accuracy_list = []\n",
    "test_loss_list      = []\n",
    "test_accuracy_list  = []\n",
    "\n",
    "train_size = 60000\n",
    "iters_num  = 100\n",
    "\n",
    "for i in range(iters_num):\n",
    "  ret = ann.train(trainData, flags=cv2.ml.ANN_MLP_UPDATE_WEIGHTS)\n",
    "\n",
    "  y_target = np.argmax(y_train, axis=1)\n",
    "  ret, res_train = ann.predict(x_train)\n",
    "  y_predict = np.argmax(res_train, axis = 1)\n",
    "  train_accuracy = np.sum(y_target==y_predict)/len(y_target)\n",
    "  train_loss = np.sum((y_train-res_train)**2)\n",
    "  train_loss /= x_train.shape[0] # 60000\n",
    "  train_accuracy_list.append(train_accuracy)\n",
    "  train_loss_list.append(train_loss)\n",
    "\n",
    "  y_target = np.argmax(y_test, axis=1)\n",
    "  ret, res_test = ann.predict(x_test)\n",
    "  y_predict = np.argmax(res_test, axis = 1)\n",
    "  test_accuracy = np.sum(y_target==y_predict)/len(y_target)\n",
    "  test_loss = np.sum((y_test-res_test)**2)\n",
    "  test_loss /= x_test.shape[0] # 10000\n",
    "  test_accuracy_list.append(test_accuracy)\n",
    "  test_loss_list.append(test_loss)\n",
    "    \n",
    "  print('train_accuracy[{}]={}, '.format(i, train_accuracy), end='')\n",
    "  print('test_accuracy={}'.format(test_accuracy))\n",
    "  \n",
    "print('train_loss={}, '.format(train_loss), end='')\n",
    "print('test_loss={}'.format(test_loss))\n",
    "  \n",
    "#4\n",
    "ann.save('./data/ann-minist_2layer_100RPROP.train')\n",
    "##ann.save('./data/ann-minist_3layer_50RPROP.train')\n",
    "\n",
    "x = list(range(len(train_loss_list)))\n",
    "plt.plot(x, train_loss_list, label='train_loss')\n",
    "plt.plot(x, test_loss_list, label='test_loss')\n",
    "\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n",
    "\n",
    "plt.plot(x, train_accuracy_list, label='train_accuracy')\n",
    "plt.plot(x, test_accuracy_list, label='test_accuracy')\n",
    "plt.legend(loc='best')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3d84c1",
   "metadata": {},
   "source": [
    "### 이전 모델을 이용한 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eec8bc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_predict= [1]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "#1\n",
    "ann = cv2.ml_ANN_MLP.load('./data/ann-minist_2layer_BP.train')\n",
    "\n",
    "#2\n",
    "def onMouse(event, x, y, flags, param):\n",
    "    if event == cv2.EVENT_MOUSEMOVE:\n",
    "        if flags & cv2.EVENT_FLAG_LBUTTON:\n",
    "            cv2.circle(dst, (x, y), 10, (255, 255, 255), -1)\n",
    "    cv2.imshow('dst', dst)\n",
    "    \n",
    "dst  = np.zeros(shape=(512, 512, 3), dtype=np.uint8)\n",
    "cv2.imshow('dst',dst)\n",
    "cv2.setMouseCallback('dst', onMouse)\n",
    "    \n",
    "mode   = cv2.RETR_EXTERNAL\n",
    "method = cv2.CHAIN_APPROX_SIMPLE\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX  \n",
    "x_img = np.zeros(shape=(28, 28), dtype=np.uint8)\n",
    "#3\n",
    "while True:\n",
    "    key = cv2.waitKey(25)    \n",
    "    if key == 27: \n",
    "        break;\n",
    "    elif key == ord('r'):\n",
    "        dst[:,:] = 0\n",
    "        cv2.imshow('dst',dst)\n",
    "    elif key == ord(' '):\n",
    "        gray = cv2.cvtColor(dst, cv2.COLOR_BGR2GRAY)\n",
    "        contours, _ = cv2.findContours(gray, mode, method)\n",
    "\n",
    "        for i, cnt in enumerate(contours):\n",
    "#3-1\n",
    "            x, y, width, height = cv2.boundingRect(cnt)\n",
    "            cv2.rectangle(dst, (x, y), (x+width, y+height), (0,0,255), 2)\n",
    "            cx, cy = x + width/2, y + height/2\n",
    "            if width > height:\n",
    "                r = width/2\n",
    "            else:\n",
    "                r = height/2            \n",
    "##            (cx,cy),r = cv2.minEnclosingCircle(cnt)\n",
    "            cx, cy, r= int(cx), int(cy), int(r)\n",
    "            img = gray[cy-r:cy+r, cx-r:cx+r]\n",
    "            img = cv2.resize(img, dsize=(20, 20),interpolation=cv2.INTER_AREA)            \n",
    "            x_img[:,:] = 0\n",
    "            x_img[4:24, 4:24] = img\n",
    "            x_img = cv2.dilate(x_img, None, 2)\n",
    "            x_img = cv2.erode(x_img, None, 4)\n",
    "            cv2.imshow('x_img', x_img)\n",
    "#3-2\n",
    "            x_test = np.float32(x_img.flatten())\n",
    "            _, res = ann.predict(x_test.reshape(-1, 784))\n",
    "##            print('res=', res)\n",
    "            y_predict = np.argmax(res, axis = 1)\n",
    "            print('y_predict=', y_predict)\n",
    "            digit = int(y_predict[0])\n",
    "            cv2.putText(dst, str(digit), (x, y), font, 3, (255,0,0), 5)\n",
    "        \n",
    "        cv2.imshow('dst',dst)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcd6717",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da4bc134",
   "metadata": {},
   "source": [
    "## 유사 이미지 검출"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821372a8",
   "metadata": {},
   "source": [
    "### Average Hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3bae61f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0]\n",
      " [0 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0]\n",
      " [0 1 0 1 1 1 1 1 1 1 1 1 0 0 0 0]\n",
      " [0 1 1 1 1 1 1 1 1 1 1 1 0 0 1 0]\n",
      " [1 1 0 0 1 1 1 1 1 1 1 1 1 0 1 0]\n",
      " [1 0 0 1 1 1 1 1 1 1 1 1 1 1 1 0]\n",
      " [0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0]\n",
      " [0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0]\n",
      " [0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1]]\n",
      "000001000180018003c003c003c087e07ff05ff07ff2cffa9ffe3ffe3ffe1fff\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "# 이미지 데이터를 Average Hash로 변환하기 --- (※1)\n",
    "def average_hash(fname, size = 16):\n",
    "    img = Image.open(fname) # 이미지 데이터 열기---(※2)\n",
    "    img = img.convert('L') # 그레이스케일로 변환하기 --- (※3)\n",
    "    img = img.resize((size, size), Image.ANTIALIAS) # 리사이즈하기 --- (※4)\n",
    "    pixel_data = img.getdata() # 픽셀 데이터 가져오기 --- (※5)\n",
    "    pixels = np.array(pixel_data) # Numpy 배열로 변환하기 --- (※6)\n",
    "    pixels = pixels.reshape((size, size)) # 2차원 배열로 변환하기 --- (※7)\n",
    "    avg = pixels.mean() # 평균 구하기 --- (※8)\n",
    "    diff = 1 * (pixels > avg) # 평균보다 크면 1, 작으면 0으로 변환하기 --- (※9)\n",
    "    return diff\n",
    "\n",
    "# 이진 해시로 변환하기 --- (※10)\n",
    "def np2hash(ahash):\n",
    "    bhash = []\n",
    "    for nl in ahash.tolist():\n",
    "        sl = [str(i) for i in nl]\n",
    "        s2 = \"\".join(sl)\n",
    "        i = int(s2, 2) # 이진수를 정수로 변환하기\n",
    "        bhash.append(\"%04x\" % i)\n",
    "    return \"\".join(bhash)\n",
    "\n",
    "# Average Hash 출력하기\n",
    "ahash = average_hash('./data/tower.jpg')\n",
    "print(ahash)\n",
    "print(np2hash(ahash))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc706f70",
   "metadata": {},
   "source": [
    "### 많은 이미지에서 유사한 이미지 검색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c54c9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 > ./data/image/chair/image_0016.jpg\n",
      "0.22265625 > ./data/image/airplanes/image_0129.jpg\n",
      "0.2265625 > ./data/image/chair/image_0031.jpg\n",
      "0.2265625 > ./data/image/stop_sign/image_0019.jpg\n",
      "0.234375 > ./data/image/umbrella/image_0009.jpg\n",
      "0.23828125 > ./data/image/airplanes/image_0124.jpg\n",
      "0.24609375 > ./data/image/dragonfly/image_0001.jpg\n",
      "0.24609375 > ./data/image/chair/image_0001.jpg\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import os, re\n",
    "# 파일 경로 지정하기\n",
    "search_dir = \"./data/image\"\n",
    "cache_dir = \"./data/image/cache_avhash\"\n",
    "if not os.path.exists(cache_dir):\n",
    "    os.mkdir(cache_dir)\n",
    "\n",
    "# 이미지 데이터를 Average Hash로 변환하기 --- (※1)\n",
    "def average_hash(fname, size = 16):\n",
    "    fname2 = fname[len(search_dir):]\n",
    "    # 이미지 캐시하기\n",
    "    cache_file = cache_dir + \"/\" + fname2.replace('/', '_') + \".csv\"\n",
    "    if not os.path.exists(cache_file): # 해시 생성하기\n",
    "        img = Image.open(fname)\n",
    "        img = img.convert('L').resize((size, size), Image.ANTIALIAS)\n",
    "        pixels = np.array(img.getdata()).reshape((size, size))\n",
    "        avg = pixels.mean()\n",
    "        px = 1 * (pixels > avg)\n",
    "        np.savetxt(cache_file, px, fmt=\"%.0f\", delimiter=\",\")\n",
    "    else: # 캐시돼 있다면 읽지 않기\n",
    "        px = np.loadtxt(cache_file, delimiter=\",\")\n",
    "    return px\n",
    "\n",
    "# 해밍 거리 구하기 --- (※2)\n",
    "def hamming_dist(a, b):\n",
    "    aa = a.reshape(1, -1) # 1차원 배열로 변환하기\n",
    "    ab = b.reshape(1, -1)\n",
    "    dist = (aa != ab).sum()\n",
    "    return dist\n",
    "\n",
    "# 모든 폴더에 처리 적용하기 --- (※3)\n",
    "def enum_all_files(path):\n",
    "    for root, dirs, files in os.walk(path):\n",
    "        for f in files:\n",
    "            fname = os.path.join(root, f)\n",
    "            if re.search(r'\\.(jpg|jpeg|png)$', fname):\n",
    "                yield fname\n",
    "\n",
    "# 이미지 찾기 --- (※4)\n",
    "def find_image(fname, rate):\n",
    "    src = average_hash(fname)\n",
    "    for fname in enum_all_files(search_dir):\n",
    "        dst = average_hash(fname)\n",
    "        diff_r = hamming_dist(src, dst) / 256\n",
    "        # print(\"[check] \",fname)\n",
    "        if diff_r < rate:\n",
    "            yield (diff_r, fname)\n",
    "\n",
    "# 찾기 --- (※5)\n",
    "srcfile = search_dir + \"/chair/image_0016.jpg\"\n",
    "html = \"\"\n",
    "sim = list(find_image(srcfile, 0.25))\n",
    "sim = sorted(sim, key=lambda x:x[0])\n",
    "for r, f in sim:\n",
    "    print(r, \">\", f)\n",
    "    s = '<div style=\"float:left;\"><h3>[ 차이 :' + str(r) + '-' + \\\n",
    "        os.path.basename(f) + ']</h3>'+ \\\n",
    "        '<p><a href=\"' + f + '\"><img src=\"' + f + '\" width=400>'+ \\\n",
    "        '</a></p></div>'\n",
    "    html += s\n",
    "\n",
    "# HTML로 출력하기\n",
    "html = \"\"\"<html><head><meta charset=\"utf8\"></head>\n",
    "<body><h3>원래 이미지</h3><p>\n",
    "<img src='{0}' width=400></p>{1}\n",
    "</body></html>\"\"\".format(srcfile, html)\n",
    "with open(\"./avhash-search-output.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(html)\n",
    "print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e353cc04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "120f4361",
   "metadata": {},
   "source": [
    "## 이미지 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183fcbe9",
   "metadata": {},
   "source": [
    "### Caltech101 이미지 분류"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8e2f22",
   "metadata": {},
   "source": [
    "#### 이미지 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52754b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 \n",
      " [[[ 40  24  31]\n",
      "  [ 33  20  24]\n",
      "  [ 27  16  20]\n",
      "  ...\n",
      "  [ 77  69  93]\n",
      "  [ 75  66  92]\n",
      "  [ 70  61  89]]\n",
      "\n",
      " [[ 40  25  32]\n",
      "  [ 34  20  24]\n",
      "  [ 31  19  23]\n",
      "  ...\n",
      "  [ 80  72  96]\n",
      "  [ 77  68  94]\n",
      "  [ 75  65  94]]\n",
      "\n",
      " [[ 41  26  33]\n",
      "  [ 35  21  25]\n",
      "  [ 33  22  26]\n",
      "  ...\n",
      "  [ 84  77 100]\n",
      "  [ 82  74 100]\n",
      "  [ 78  69  97]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[189 162 192]\n",
      "  [189 169 198]\n",
      "  [175 160 191]\n",
      "  ...\n",
      "  [204 173 206]\n",
      "  [202 170 204]\n",
      "  [196 164 198]]\n",
      "\n",
      " [[154 131 164]\n",
      "  [141 121 153]\n",
      "  [137 119 150]\n",
      "  ...\n",
      "  [200 170 204]\n",
      "  [195 165 200]\n",
      "  [191 161 195]]\n",
      "\n",
      " [[129 111 145]\n",
      "  [165 143 178]\n",
      "  [184 157 195]\n",
      "  ...\n",
      "  [198 168 202]\n",
      "  [199 169 203]\n",
      "  [196 166 200]]]\n",
      "10 \n",
      " [[[7 7 7]\n",
      "  [4 4 4]\n",
      "  [3 3 3]\n",
      "  ...\n",
      "  [4 4 4]\n",
      "  [4 4 4]\n",
      "  [8 8 8]]\n",
      "\n",
      " [[4 4 4]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [1 1 1]\n",
      "  [4 4 4]]\n",
      "\n",
      " [[4 4 4]\n",
      "  [1 1 1]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [2 2 2]\n",
      "  [3 3 3]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[4 4 4]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [2 2 2]\n",
      "  [3 3 3]]\n",
      "\n",
      " [[4 4 4]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [2 2 2]\n",
      "  [3 3 3]]\n",
      "\n",
      " [[7 7 7]\n",
      "  [4 4 4]\n",
      "  [3 3 3]\n",
      "  ...\n",
      "  [4 4 4]\n",
      "  [5 5 5]\n",
      "  [9 9 9]]]\n",
      "20 \n",
      " [[[63 10 28]\n",
      "  [63 10 28]\n",
      "  [66 13 31]\n",
      "  ...\n",
      "  [71 18 36]\n",
      "  [70 17 35]\n",
      "  [72 19 37]]\n",
      "\n",
      " [[67 14 32]\n",
      "  [67 14 32]\n",
      "  [71 18 36]\n",
      "  ...\n",
      "  [74 21 39]\n",
      "  [74 21 39]\n",
      "  [72 19 37]]\n",
      "\n",
      " [[73 20 38]\n",
      "  [73 20 38]\n",
      "  [73 20 38]\n",
      "  ...\n",
      "  [74 21 39]\n",
      "  [73 20 38]\n",
      "  [72 18 36]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 3  3  3]\n",
      "  [ 3  3  3]\n",
      "  [ 3  3  3]\n",
      "  ...\n",
      "  [ 3  3  3]\n",
      "  [ 3  3  3]\n",
      "  [ 3  3  3]]\n",
      "\n",
      " [[ 3  3  3]\n",
      "  [ 3  3  3]\n",
      "  [ 3  3  3]\n",
      "  ...\n",
      "  [ 3  3  3]\n",
      "  [ 3  3  3]\n",
      "  [ 3  3  3]]\n",
      "\n",
      " [[ 3  3  3]\n",
      "  [ 3  3  3]\n",
      "  [ 3  3  3]\n",
      "  ...\n",
      "  [ 3  3  3]\n",
      "  [ 3  3  3]\n",
      "  [ 3  3  3]]]\n",
      "30 \n",
      " [[[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]]\n",
      "40 \n",
      " [[[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]]\n",
      "50 \n",
      " [[[207 187 188]\n",
      "  [207 187 188]\n",
      "  [207 187 186]\n",
      "  ...\n",
      "  [204 180 154]\n",
      "  [201 177 151]\n",
      "  [200 176 150]]\n",
      "\n",
      " [[207 187 186]\n",
      "  [207 187 185]\n",
      "  [207 188 184]\n",
      "  ...\n",
      "  [207 183 157]\n",
      "  [204 180 154]\n",
      "  [202 178 152]]\n",
      "\n",
      " [[205 186 181]\n",
      "  [205 186 180]\n",
      "  [205 186 179]\n",
      "  ...\n",
      "  [207 183 157]\n",
      "  [204 180 154]\n",
      "  [202 178 152]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[232 220 208]\n",
      "  [231 219 206]\n",
      "  [229 216 203]\n",
      "  ...\n",
      "  [252 251 229]\n",
      "  [252 251 227]\n",
      "  [252 251 227]]\n",
      "\n",
      " [[236 223 215]\n",
      "  [235 222 213]\n",
      "  [233 220 209]\n",
      "  ...\n",
      "  [255 253 232]\n",
      "  [255 253 232]\n",
      "  [255 253 232]]\n",
      "\n",
      " [[233 220 212]\n",
      "  [232 219 210]\n",
      "  [232 219 208]\n",
      "  ...\n",
      "  [255 252 234]\n",
      "  [255 252 234]\n",
      "  [255 252 234]]]\n",
      "60 \n",
      " [[[183 178 184]\n",
      "  [182 177 183]\n",
      "  [181 176 182]\n",
      "  ...\n",
      "  [181 177 178]\n",
      "  [180 176 177]\n",
      "  [178 174 175]]\n",
      "\n",
      " [[183 178 184]\n",
      "  [185 180 186]\n",
      "  [184 179 185]\n",
      "  ...\n",
      "  [183 179 180]\n",
      "  [183 179 180]\n",
      "  [181 177 178]]\n",
      "\n",
      " [[184 179 185]\n",
      "  [187 182 189]\n",
      "  [186 181 187]\n",
      "  ...\n",
      "  [184 180 181]\n",
      "  [183 179 180]\n",
      "  [183 179 180]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[246 246 246]\n",
      "  [246 246 246]\n",
      "  [246 246 246]\n",
      "  ...\n",
      "  [246 246 246]\n",
      "  [246 246 246]\n",
      "  [246 246 246]]\n",
      "\n",
      " [[246 246 246]\n",
      "  [246 246 246]\n",
      "  [246 246 246]\n",
      "  ...\n",
      "  [246 246 246]\n",
      "  [246 246 246]\n",
      "  [246 246 246]]\n",
      "\n",
      " [[246 246 246]\n",
      "  [246 246 246]\n",
      "  [246 246 246]\n",
      "  ...\n",
      "  [247 247 247]\n",
      "  [246 246 246]\n",
      "  [246 246 246]]]\n",
      "0 \n",
      " [[[239 235 236]\n",
      "  [239 235 236]\n",
      "  [239 235 236]\n",
      "  ...\n",
      "  [229 225 226]\n",
      "  [228 224 225]\n",
      "  [228 224 225]]\n",
      "\n",
      " [[239 235 236]\n",
      "  [239 235 236]\n",
      "  [239 235 236]\n",
      "  ...\n",
      "  [230 226 227]\n",
      "  [230 226 227]\n",
      "  [230 226 227]]\n",
      "\n",
      " [[238 234 235]\n",
      "  [238 234 235]\n",
      "  [238 234 235]\n",
      "  ...\n",
      "  [230 226 227]\n",
      "  [230 226 227]\n",
      "  [230 226 227]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[238 234 231]\n",
      "  [238 234 231]\n",
      "  [238 234 231]\n",
      "  ...\n",
      "  [233 229 226]\n",
      "  [231 227 224]\n",
      "  [232 228 225]]\n",
      "\n",
      " [[238 234 231]\n",
      "  [238 234 231]\n",
      "  [238 234 231]\n",
      "  ...\n",
      "  [232 228 225]\n",
      "  [232 228 225]\n",
      "  [232 228 225]]\n",
      "\n",
      " [[238 234 231]\n",
      "  [238 234 231]\n",
      "  [238 234 231]\n",
      "  ...\n",
      "  [232 228 225]\n",
      "  [232 228 225]\n",
      "  [232 228 225]]]\n",
      "10 \n",
      " [[[247 247 247]\n",
      "  [247 247 247]\n",
      "  [247 247 247]\n",
      "  ...\n",
      "  [247 247 247]\n",
      "  [247 247 247]\n",
      "  [247 247 247]]\n",
      "\n",
      " [[247 247 247]\n",
      "  [247 247 247]\n",
      "  [247 247 247]\n",
      "  ...\n",
      "  [247 247 247]\n",
      "  [247 247 247]\n",
      "  [247 247 247]]\n",
      "\n",
      " [[247 247 247]\n",
      "  [247 247 247]\n",
      "  [247 247 247]\n",
      "  ...\n",
      "  [247 247 247]\n",
      "  [247 247 247]\n",
      "  [247 247 247]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[233 234 239]\n",
      "  [234 234 239]\n",
      "  [232 233 235]\n",
      "  ...\n",
      "  [233 232 235]\n",
      "  [232 232 234]\n",
      "  [233 233 235]]\n",
      "\n",
      " [[236 237 244]\n",
      "  [236 237 242]\n",
      "  [233 234 237]\n",
      "  ...\n",
      "  [233 232 234]\n",
      "  [232 232 234]\n",
      "  [234 234 236]]\n",
      "\n",
      " [[234 237 245]\n",
      "  [234 237 244]\n",
      "  [232 237 240]\n",
      "  ...\n",
      "  [234 233 235]\n",
      "  [233 233 235]\n",
      "  [236 236 238]]]\n",
      "20 \n",
      " [[[251 251 251]\n",
      "  [251 251 251]\n",
      "  [251 251 251]\n",
      "  ...\n",
      "  [249 249 249]\n",
      "  [251 251 251]\n",
      "  [252 252 252]]\n",
      "\n",
      " [[251 251 251]\n",
      "  [251 251 251]\n",
      "  [251 251 251]\n",
      "  ...\n",
      "  [250 250 250]\n",
      "  [251 251 251]\n",
      "  [251 251 251]]\n",
      "\n",
      " [[252 252 252]\n",
      "  [252 252 252]\n",
      "  [252 252 252]\n",
      "  ...\n",
      "  [250 250 250]\n",
      "  [249 249 249]\n",
      "  [251 251 251]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[251 251 251]\n",
      "  [251 251 251]\n",
      "  [252 252 252]\n",
      "  ...\n",
      "  [251 251 251]\n",
      "  [250 250 250]\n",
      "  [251 251 251]]\n",
      "\n",
      " [[250 250 250]\n",
      "  [250 250 250]\n",
      "  [250 250 250]\n",
      "  ...\n",
      "  [251 251 251]\n",
      "  [251 251 251]\n",
      "  [251 251 251]]\n",
      "\n",
      " [[251 251 251]\n",
      "  [251 251 251]\n",
      "  [250 250 250]\n",
      "  ...\n",
      "  [250 250 250]\n",
      "  [250 250 250]\n",
      "  [249 249 249]]]\n",
      "30 \n",
      " [[[254 254 254]\n",
      "  [254 254 254]\n",
      "  [254 254 254]\n",
      "  ...\n",
      "  [254 254 254]\n",
      "  [254 254 254]\n",
      "  [254 254 254]]\n",
      "\n",
      " [[254 254 254]\n",
      "  [254 254 254]\n",
      "  [254 254 254]\n",
      "  ...\n",
      "  [254 254 254]\n",
      "  [254 254 254]\n",
      "  [254 254 254]]\n",
      "\n",
      " [[254 254 254]\n",
      "  [254 254 254]\n",
      "  [254 254 254]\n",
      "  ...\n",
      "  [254 254 254]\n",
      "  [254 254 254]\n",
      "  [254 254 254]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[254 254 254]\n",
      "  [254 254 254]\n",
      "  [254 254 254]\n",
      "  ...\n",
      "  [254 254 254]\n",
      "  [254 254 254]\n",
      "  [254 254 254]]\n",
      "\n",
      " [[254 254 254]\n",
      "  [254 254 254]\n",
      "  [254 254 254]\n",
      "  ...\n",
      "  [254 254 254]\n",
      "  [254 254 254]\n",
      "  [254 254 254]]\n",
      "\n",
      " [[254 254 254]\n",
      "  [254 254 254]\n",
      "  [254 254 254]\n",
      "  ...\n",
      "  [254 254 254]\n",
      "  [254 254 254]\n",
      "  [254 254 254]]]\n",
      "40 \n",
      " [[[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]]\n",
      "0 \n",
      " [[[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]]]\n",
      "10 \n",
      " [[[118 168 169]\n",
      "  [116 166 167]\n",
      "  [118 168 169]\n",
      "  ...\n",
      "  [118 168 169]\n",
      "  [118 168 169]\n",
      "  [116 166 167]]\n",
      "\n",
      " [[116 166 167]\n",
      "  [117 167 168]\n",
      "  [115 165 166]\n",
      "  ...\n",
      "  [122 172 173]\n",
      "  [119 169 170]\n",
      "  [116 166 167]]\n",
      "\n",
      " [[113 163 164]\n",
      "  [116 166 167]\n",
      "  [107 157 158]\n",
      "  ...\n",
      "  [125 175 176]\n",
      "  [114 164 165]\n",
      "  [120 170 171]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[115 165 164]\n",
      "  [117 163 163]\n",
      "  [116 169 167]\n",
      "  ...\n",
      "  [116 165 165]\n",
      "  [114 165 165]\n",
      "  [113 163 162]]\n",
      "\n",
      " [[124 172 171]\n",
      "  [119 165 165]\n",
      "  [110 166 163]\n",
      "  ...\n",
      "  [115 165 164]\n",
      "  [117 168 165]\n",
      "  [126 173 171]]\n",
      "\n",
      " [[126 171 172]\n",
      "  [125 171 171]\n",
      "  [113 171 167]\n",
      "  ...\n",
      "  [117 168 166]\n",
      "  [122 173 168]\n",
      "  [124 169 166]]]\n",
      "20 \n",
      " [[[121 127 123]\n",
      "  [125 129 125]\n",
      "  [124 124 122]\n",
      "  ...\n",
      "  [104 123  93]\n",
      "  [107 126 106]\n",
      "  [108 126 115]]\n",
      "\n",
      " [[123 129 125]\n",
      "  [128 132 128]\n",
      "  [126 126 124]\n",
      "  ...\n",
      "  [108 126 101]\n",
      "  [108 127 109]\n",
      "  [109 126 114]]\n",
      "\n",
      " [[121 128 122]\n",
      "  [129 133 127]\n",
      "  [128 129 125]\n",
      "  ...\n",
      "  [111 126 109]\n",
      "  [111 128 112]\n",
      "  [107 124 107]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[234  84   8]\n",
      "  [237  89   5]\n",
      "  [221  86   7]\n",
      "  ...\n",
      "  [171 151 150]\n",
      "  [146 135 129]\n",
      "  [116 116 106]]\n",
      "\n",
      " [[226  82   7]\n",
      "  [236  92   6]\n",
      "  [216  84   7]\n",
      "  ...\n",
      "  [153 137 138]\n",
      "  [132 126 119]\n",
      "  [112 117 107]]\n",
      "\n",
      " [[223  80   7]\n",
      "  [239  91   7]\n",
      "  [214  82   7]\n",
      "  ...\n",
      "  [142 134 127]\n",
      "  [126 125 114]\n",
      "  [112 117 104]]]\n",
      "30 \n",
      " [[[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 253 255]\n",
      "  [253 254 253]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [237 236 237]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [196 198 197]\n",
      "  [240 240 240]\n",
      "  [254 254 254]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]]\n",
      "40 \n",
      " [[[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]\n",
      "\n",
      " [[0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  ...\n",
      "  [0 0 0]\n",
      "  [0 0 0]\n",
      "  [0 0 0]]]\n",
      "50 \n",
      " [[[ 82 117  94]\n",
      "  [ 87 117  91]\n",
      "  [ 83 111  85]\n",
      "  ...\n",
      "  [ 37  63  16]\n",
      "  [ 17  27   3]\n",
      "  [ 17  17  10]]\n",
      "\n",
      " [[ 87 113  95]\n",
      "  [ 80 111  88]\n",
      "  [ 72 108  76]\n",
      "  ...\n",
      "  [ 32  57  11]\n",
      "  [ 18  30   4]\n",
      "  [ 14  16   7]]\n",
      "\n",
      " [[ 86 107  84]\n",
      "  [ 75 104  86]\n",
      "  [ 75 104  86]\n",
      "  ...\n",
      "  [ 31  55  10]\n",
      "  [ 18  32   3]\n",
      "  [ 13  17   5]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[124 140  98]\n",
      "  [120 108  44]\n",
      "  [121 115   7]\n",
      "  ...\n",
      "  [212 205  11]\n",
      "  [168 172  50]\n",
      "  [ 78  96  54]]\n",
      "\n",
      " [[125 136  87]\n",
      "  [128 122  30]\n",
      "  [133 129   5]\n",
      "  ...\n",
      "  [207 200  30]\n",
      "  [119 130  24]\n",
      "  [ 71  94  57]]\n",
      "\n",
      " [[128 137  53]\n",
      "  [139 141   6]\n",
      "  [138 140   5]\n",
      "  ...\n",
      "  [188 180  15]\n",
      "  [137 143  28]\n",
      "  [ 89 101  81]]]\n",
      "60 \n",
      " [[[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 253 254]\n",
      "  [255 253 254]\n",
      "  [254 254 254]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [253 254 255]\n",
      "  [254 254 254]\n",
      "  [254 254 254]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [253 254 254]\n",
      "  [253 255 254]\n",
      "  [254 253 254]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]]\n",
      "70 \n",
      " [[[  0   0   0]\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]\n",
      "  ...\n",
      "  [  0   0   1]\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]]\n",
      "\n",
      " [[  0   0   0]\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]\n",
      "  ...\n",
      "  [  0   0   1]\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]]\n",
      "\n",
      " [[  0   0   0]\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]\n",
      "  ...\n",
      "  [ 12  10  10]\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[240 225 207]\n",
      "  [211 198 195]\n",
      "  [172 158 148]\n",
      "  ...\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]]\n",
      "\n",
      " [[234 220 206]\n",
      "  [237 226 218]\n",
      "  [198 185 178]\n",
      "  ...\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]]\n",
      "\n",
      " [[166 157 148]\n",
      "  [239 227 213]\n",
      "  [219 208 202]\n",
      "  ...\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80 \n",
      " [[[ 55  49  42]\n",
      "  [ 58  51  43]\n",
      "  [ 75  63  65]\n",
      "  ...\n",
      "  [209 163 162]\n",
      "  [184 147 137]\n",
      "  [185 161 144]]\n",
      "\n",
      " [[ 53  56  37]\n",
      "  [ 47  49  29]\n",
      "  [ 61  59  48]\n",
      "  ...\n",
      "  [126 101  80]\n",
      "  [143 110 105]\n",
      "  [225 193 202]]\n",
      "\n",
      " [[ 80  85  61]\n",
      "  [ 73  77  52]\n",
      "  [ 91  91  75]\n",
      "  ...\n",
      "  [146 148 101]\n",
      "  [190 163 165]\n",
      "  [246 205 237]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[  0   0   0]\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]\n",
      "  ...\n",
      "  [135 150 105]\n",
      "  [116 123  88]\n",
      "  [102 109  78]]\n",
      "\n",
      " [[  0   0   0]\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]\n",
      "  ...\n",
      "  [ 67  79  42]\n",
      "  [ 69  78  44]\n",
      "  [ 74  82  52]]\n",
      "\n",
      " [[  0   0   0]\n",
      "  [  0   0   0]\n",
      "  [  0   0   0]\n",
      "  ...\n",
      "  [ 71  83  47]\n",
      "  [ 71  83  45]\n",
      "  [ 70  83  44]]]\n",
      "90 \n",
      " [[[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]\n",
      "\n",
      " [[255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  ...\n",
      "  [255 255 255]\n",
      "  [255 255 255]\n",
      "  [255 255 255]]]\n",
      "0 \n",
      " [[[145 143 123]\n",
      "  [145 141 111]\n",
      "  [147 151 116]\n",
      "  ...\n",
      "  [ 29  35  11]\n",
      "  [ 31  35  20]\n",
      "  [ 28  31  23]]\n",
      "\n",
      " [[114 112  90]\n",
      "  [123 118  87]\n",
      "  [123 125  89]\n",
      "  ...\n",
      "  [ 54  59  38]\n",
      "  [ 37  40  28]\n",
      "  [ 34  37  29]]\n",
      "\n",
      " [[ 94  89  66]\n",
      "  [111 104  71]\n",
      "  [124 123  86]\n",
      "  ...\n",
      "  [ 36  39  29]\n",
      "  [ 36  38  32]\n",
      "  [ 39  41  36]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 72  45  36]\n",
      "  [ 74  45  34]\n",
      "  [ 95  74  57]\n",
      "  ...\n",
      "  [252 247 232]\n",
      "  [251 248 230]\n",
      "  [254 251 233]]\n",
      "\n",
      " [[ 64  42  38]\n",
      "  [118  92  83]\n",
      "  [138 118  99]\n",
      "  ...\n",
      "  [250 245 222]\n",
      "  [249 247 221]\n",
      "  [251 248 223]]\n",
      "\n",
      " [[ 96  77  69]\n",
      "  [138 112 102]\n",
      "  [126 102  85]\n",
      "  ...\n",
      "  [249 237 208]\n",
      "  [246 236 207]\n",
      "  [241 230 203]]]\n",
      "10 \n",
      " [[[111 132 159]\n",
      "  [113 134 161]\n",
      "  [114 135 162]\n",
      "  ...\n",
      "  [ 64  81 105]\n",
      "  [ 60  77 105]\n",
      "  [ 59  76 104]]\n",
      "\n",
      " [[106 126 153]\n",
      "  [107 126 153]\n",
      "  [109 129 156]\n",
      "  ...\n",
      "  [ 60  77 102]\n",
      "  [ 59  76 104]\n",
      "  [ 59  76 104]]\n",
      "\n",
      " [[105 121 150]\n",
      "  [106 122 151]\n",
      "  [110 126 154]\n",
      "  ...\n",
      "  [ 58  76 102]\n",
      "  [ 59  76 104]\n",
      "  [ 59  76 104]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 56  62  36]\n",
      "  [ 58  64  38]\n",
      "  [ 60  66  40]\n",
      "  ...\n",
      "  [ 83  84  46]\n",
      "  [ 80  81  49]\n",
      "  [ 48  49  35]]\n",
      "\n",
      " [[ 60  66  40]\n",
      "  [ 61  67  41]\n",
      "  [ 69  75  49]\n",
      "  ...\n",
      "  [ 86  84  50]\n",
      "  [ 79  79  53]\n",
      "  [ 31  31  20]]\n",
      "\n",
      " [[ 48  54  30]\n",
      "  [ 55  61  37]\n",
      "  [ 51  57  33]\n",
      "  ...\n",
      "  [ 68  65  37]\n",
      "  [ 42  41  27]\n",
      "  [  8   8   3]]]\n",
      "20 \n",
      " [[[106 104  74]\n",
      "  [ 51  60  50]\n",
      "  [ 36  39  39]\n",
      "  ...\n",
      "  [ 44  48  51]\n",
      "  [ 46  49  54]\n",
      "  [ 43  46  51]]\n",
      "\n",
      " [[110 109  77]\n",
      "  [ 80  89  71]\n",
      "  [ 45  48  50]\n",
      "  ...\n",
      "  [ 42  45  49]\n",
      "  [ 36  39  44]\n",
      "  [ 34  37  42]]\n",
      "\n",
      " [[156 156 127]\n",
      "  [ 74  81  52]\n",
      "  [ 43  45  45]\n",
      "  ...\n",
      "  [ 32  35  40]\n",
      "  [ 26  29  34]\n",
      "  [ 23  26  31]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[139 110 101]\n",
      "  [139 110 103]\n",
      "  [111  82  78]\n",
      "  ...\n",
      "  [160 137 155]\n",
      "  [158 135 152]\n",
      "  [158 135 152]]\n",
      "\n",
      " [[ 70  52  48]\n",
      "  [ 53  35  32]\n",
      "  [ 60  41  39]\n",
      "  ...\n",
      "  [198 177 189]\n",
      "  [185 163 177]\n",
      "  [178 156 169]]\n",
      "\n",
      " [[ 52  37  43]\n",
      "  [ 58  42  44]\n",
      "  [ 84  66  64]\n",
      "  ...\n",
      "  [235 219 224]\n",
      "  [232 215 221]\n",
      "  [231 212 218]]]\n",
      "30 \n",
      " [[[191 208 215]\n",
      "  [160 177 184]\n",
      "  [191 208 216]\n",
      "  ...\n",
      "  [189 205 177]\n",
      "  [210 215 191]\n",
      "  [234 227 211]]\n",
      "\n",
      " [[187 205 213]\n",
      "  [155 173 181]\n",
      "  [183 200 208]\n",
      "  ...\n",
      "  [171 188 157]\n",
      "  [202 206 183]\n",
      "  [235 230 211]]\n",
      "\n",
      " [[181 200 211]\n",
      "  [156 175 182]\n",
      "  [179 199 203]\n",
      "  ...\n",
      "  [158 175 144]\n",
      "  [199 204 181]\n",
      "  [233 228 206]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[174 151 110]\n",
      "  [171 147 106]\n",
      "  [182 160 118]\n",
      "  ...\n",
      "  [228 207 166]\n",
      "  [244 230 201]\n",
      "  [245 235 214]]\n",
      "\n",
      " [[211 191 151]\n",
      "  [197 177 137]\n",
      "  [213 193 153]\n",
      "  ...\n",
      "  [226 208 173]\n",
      "  [240 227 201]\n",
      "  [242 235 213]]\n",
      "\n",
      " [[238 219 183]\n",
      "  [242 223 187]\n",
      "  [239 219 183]\n",
      "  ...\n",
      "  [226 211 177]\n",
      "  [246 233 206]\n",
      "  [243 235 214]]]\n",
      "40 \n",
      " [[[234 238 246]\n",
      "  [224 230 240]\n",
      "  [218 229 239]\n",
      "  ...\n",
      "  [194 208 246]\n",
      "  [186 204 248]\n",
      "  [174 200 245]]\n",
      "\n",
      " [[221 228 251]\n",
      "  [212 221 245]\n",
      "  [204 214 242]\n",
      "  ...\n",
      "  [167 189 234]\n",
      "  [169 190 238]\n",
      "  [161 178 240]]\n",
      "\n",
      " [[201 219 238]\n",
      "  [205 222 244]\n",
      "  [204 219 245]\n",
      "  ...\n",
      "  [160 182 236]\n",
      "  [181 200 239]\n",
      "  [159 170 229]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[135 124  68]\n",
      "  [135 124  68]\n",
      "  [136 125  69]\n",
      "  ...\n",
      "  [147 126  70]\n",
      "  [134 113  58]\n",
      "  [139 118  63]]\n",
      "\n",
      " [[139 128  72]\n",
      "  [136 125  69]\n",
      "  [135 124  68]\n",
      "  ...\n",
      "  [156 135  80]\n",
      "  [133 112  57]\n",
      "  [140 119  64]]\n",
      "\n",
      " [[137 126  70]\n",
      "  [134 123  67]\n",
      "  [135 124  68]\n",
      "  ...\n",
      "  [155 134  80]\n",
      "  [138 117  62]\n",
      "  [134 113  58]]]\n",
      "50 \n",
      " [[[178 185 203]\n",
      "  [181 188 206]\n",
      "  [184 192 210]\n",
      "  ...\n",
      "  [208 218 223]\n",
      "  [207 218 224]\n",
      "  [206 217 223]]\n",
      "\n",
      " [[176 186 203]\n",
      "  [180 189 206]\n",
      "  [182 192 208]\n",
      "  ...\n",
      "  [208 217 222]\n",
      "  [207 218 224]\n",
      "  [206 217 223]]\n",
      "\n",
      " [[174 186 202]\n",
      "  [177 190 206]\n",
      "  [179 191 208]\n",
      "  ...\n",
      "  [208 218 222]\n",
      "  [207 218 224]\n",
      "  [205 216 222]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[154 118  98]\n",
      "  [155 118  98]\n",
      "  [154 118  98]\n",
      "  ...\n",
      "  [206 161 139]\n",
      "  [203 160 139]\n",
      "  [183 141 120]]\n",
      "\n",
      " [[158 121 103]\n",
      "  [164 127 108]\n",
      "  [163 127 108]\n",
      "  ...\n",
      "  [196 150 130]\n",
      "  [202 153 137]\n",
      "  [197 148 132]]\n",
      "\n",
      " [[139 103  84]\n",
      "  [153 116  98]\n",
      "  [155 116 100]\n",
      "  ...\n",
      "  [204 155 139]\n",
      "  [199 149 135]\n",
      "  [198 148 134]]]\n",
      "60 \n",
      " [[[ 54  59  48]\n",
      "  [ 81  92  66]\n",
      "  [ 92  93  73]\n",
      "  ...\n",
      "  [ 98  86  78]\n",
      "  [ 97  85  73]\n",
      "  [ 72  65  57]]\n",
      "\n",
      " [[110 118  90]\n",
      "  [161 171 127]\n",
      "  [172 174 136]\n",
      "  ...\n",
      "  [171 154 136]\n",
      "  [171 154 133]\n",
      "  [120 108  94]]\n",
      "\n",
      " [[105 110  87]\n",
      "  [155 154 124]\n",
      "  [149 144 115]\n",
      "  ...\n",
      "  [170 150 127]\n",
      "  [174 154 129]\n",
      "  [127 111  94]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 98 102  80]\n",
      "  [139 148 104]\n",
      "  [121 132  94]\n",
      "  ...\n",
      "  [125 127 117]\n",
      "  [124 126 111]\n",
      "  [ 85  87  76]]\n",
      "\n",
      " [[ 87  89  74]\n",
      "  [146 155 115]\n",
      "  [142 153 115]\n",
      "  ...\n",
      "  [133 134 128]\n",
      "  [122 124 113]\n",
      "  [ 82  84  74]]\n",
      "\n",
      " [[ 59  59  52]\n",
      "  [ 82  87  61]\n",
      "  [ 81  89  63]\n",
      "  ...\n",
      "  [ 74  74  72]\n",
      "  [ 71  73  66]\n",
      "  [ 52  53  46]]]\n",
      "0 \n",
      " [[[100 103 138]\n",
      "  [100  98 146]\n",
      "  [ 89 109 136]\n",
      "  ...\n",
      "  [104 108 137]\n",
      "  [100 104 133]\n",
      "  [ 99 103 132]]\n",
      "\n",
      " [[ 90 106 141]\n",
      "  [ 96 101 143]\n",
      "  [109  94 117]\n",
      "  ...\n",
      "  [ 97 101 130]\n",
      "  [100 104 133]\n",
      "  [101 105 134]]\n",
      "\n",
      " [[ 98 107 139]\n",
      "  [103 101 131]\n",
      "  [171 106 118]\n",
      "  ...\n",
      "  [ 97 101 130]\n",
      "  [101 104 134]\n",
      "  [101 104 134]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[120 131 178]\n",
      "  [116 129 178]\n",
      "  [114 129 179]\n",
      "  ...\n",
      "  [121 140 190]\n",
      "  [120 140 190]\n",
      "  [123 143 193]]\n",
      "\n",
      " [[120 132 178]\n",
      "  [119 133 181]\n",
      "  [118 134 184]\n",
      "  ...\n",
      "  [122 138 189]\n",
      "  [124 140 191]\n",
      "  [124 139 190]]\n",
      "\n",
      " [[122 138 183]\n",
      "  [124 140 188]\n",
      "  [121 140 189]\n",
      "  ...\n",
      "  [129 142 194]\n",
      "  [130 143 195]\n",
      "  [126 138 190]]]\n",
      "10 \n",
      " [[[  8   6   7]\n",
      "  [  9   8   8]\n",
      "  [ 34  35  34]\n",
      "  ...\n",
      "  [  8   8   6]\n",
      "  [  8   8   5]\n",
      "  [  8   9   4]]\n",
      "\n",
      " [[  7   9   8]\n",
      "  [ 14  16  15]\n",
      "  [ 21  20  21]\n",
      "  ...\n",
      "  [  8   8   6]\n",
      "  [  8   9   5]\n",
      "  [  8   9   4]]\n",
      "\n",
      " [[ 11  18  16]\n",
      "  [ 10  14  13]\n",
      "  [  9   6   7]\n",
      "  ...\n",
      "  [  8   8   6]\n",
      "  [  8   9   5]\n",
      "  [  8   9   4]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 76  94  67]\n",
      "  [ 81 102  72]\n",
      "  [ 77  99  70]\n",
      "  ...\n",
      "  [ 67  87  62]\n",
      "  [ 63  83  60]\n",
      "  [ 66  87  63]]\n",
      "\n",
      " [[ 71  91  64]\n",
      "  [ 70  91  60]\n",
      "  [ 68  88  59]\n",
      "  ...\n",
      "  [ 89 108  84]\n",
      "  [ 84 105  84]\n",
      "  [ 84 106  85]]\n",
      "\n",
      " [[ 68  88  61]\n",
      "  [ 68  89  58]\n",
      "  [ 66  87  58]\n",
      "  ...\n",
      "  [ 85 104  80]\n",
      "  [ 72  93  72]\n",
      "  [ 81 103  82]]]\n",
      "20 \n",
      " [[[ 91 116 145]\n",
      "  [ 90 116 143]\n",
      "  [ 91 114 138]\n",
      "  ...\n",
      "  [ 16  14   8]\n",
      "  [ 12  11   8]\n",
      "  [  9  10   7]]\n",
      "\n",
      " [[ 95 118 151]\n",
      "  [ 94 116 147]\n",
      "  [ 92 113 140]\n",
      "  ...\n",
      "  [ 10  10   5]\n",
      "  [  8  10   4]\n",
      "  [  5   8   2]]\n",
      "\n",
      " [[ 94 115 141]\n",
      "  [ 97 117 141]\n",
      "  [ 98 116 136]\n",
      "  ...\n",
      "  [ 19  19  22]\n",
      "  [ 18  20  29]\n",
      "  [ 31  34  47]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[127 140 127]\n",
      "  [127 139 127]\n",
      "  [131 143 132]\n",
      "  ...\n",
      "  [ 82 102 130]\n",
      "  [ 79  97 122]\n",
      "  [ 71  84 106]]\n",
      "\n",
      " [[137 143 130]\n",
      "  [137 142 131]\n",
      "  [137 142 132]\n",
      "  ...\n",
      "  [ 83 100 107]\n",
      "  [ 73  87  93]\n",
      "  [ 71  80  86]]\n",
      "\n",
      " [[133 139 106]\n",
      "  [132 139 107]\n",
      "  [132 138 108]\n",
      "  ...\n",
      "  [ 86  93  96]\n",
      "  [ 80  85  91]\n",
      "  [ 81  87  94]]]\n",
      "30 \n",
      " [[[21 24 31]\n",
      "  [17 20 26]\n",
      "  [19 23 26]\n",
      "  ...\n",
      "  [20 18 17]\n",
      "  [32 26 19]\n",
      "  [72 60 47]]\n",
      "\n",
      " [[28 31 36]\n",
      "  [20 23 27]\n",
      "  [20 24 25]\n",
      "  ...\n",
      "  [20 18 16]\n",
      "  [26 20 15]\n",
      "  [69 59 48]]\n",
      "\n",
      " [[24 28 30]\n",
      "  [25 29 30]\n",
      "  [22 26 25]\n",
      "  ...\n",
      "  [31 26 24]\n",
      "  [27 21 16]\n",
      "  [56 50 41]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[25 25 22]\n",
      "  [24 24 21]\n",
      "  [27 26 23]\n",
      "  ...\n",
      "  [24 24 22]\n",
      "  [30 30 28]\n",
      "  [38 38 36]]\n",
      "\n",
      " [[28 24 23]\n",
      "  [27 23 22]\n",
      "  [29 24 24]\n",
      "  ...\n",
      "  [27 27 25]\n",
      "  [29 29 27]\n",
      "  [32 32 30]]\n",
      "\n",
      " [[40 36 35]\n",
      "  [38 34 33]\n",
      "  [41 37 36]\n",
      "  ...\n",
      "  [38 38 36]\n",
      "  [38 38 36]\n",
      "  [38 38 36]]]\n",
      "40 \n",
      " [[[232 231 236]\n",
      "  [233 232 237]\n",
      "  [234 233 238]\n",
      "  ...\n",
      "  [137 141 111]\n",
      "  [173 171 145]\n",
      "  [156 149 133]]\n",
      "\n",
      " [[231 230 235]\n",
      "  [231 230 235]\n",
      "  [234 233 238]\n",
      "  ...\n",
      "  [147 150 125]\n",
      "  [156 153 132]\n",
      "  [165 157 140]]\n",
      "\n",
      " [[233 232 237]\n",
      "  [233 232 237]\n",
      "  [234 233 238]\n",
      "  ...\n",
      "  [140 143 124]\n",
      "  [154 150 134]\n",
      "  [184 177 158]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[227 226 231]\n",
      "  [227 226 231]\n",
      "  [226 225 230]\n",
      "  ...\n",
      "  [204 205 215]\n",
      "  [202 202 212]\n",
      "  [202 201 212]]\n",
      "\n",
      " [[228 227 232]\n",
      "  [228 227 232]\n",
      "  [227 226 231]\n",
      "  ...\n",
      "  [199 197 208]\n",
      "  [198 196 207]\n",
      "  [195 193 204]]\n",
      "\n",
      " [[196 195 200]\n",
      "  [199 198 203]\n",
      "  [200 199 204]\n",
      "  ...\n",
      "  [197 195 206]\n",
      "  [189 187 198]\n",
      "  [182 180 191]]]\n",
      "50 \n",
      " [[[ 0  1  1]\n",
      "  [ 1  1  0]\n",
      "  [ 1  1  1]\n",
      "  ...\n",
      "  [ 0  0  0]\n",
      "  [ 0  0  0]\n",
      "  [ 0  0  0]]\n",
      "\n",
      " [[ 4  0  0]\n",
      "  [ 1  0  3]\n",
      "  [ 1  1  6]\n",
      "  ...\n",
      "  [ 0  0  0]\n",
      "  [ 0  0  0]\n",
      "  [ 0  0  0]]\n",
      "\n",
      " [[ 3  0  0]\n",
      "  [ 1  2  5]\n",
      "  [17 14 14]\n",
      "  ...\n",
      "  [ 0  0  0]\n",
      "  [ 0  0  0]\n",
      "  [ 0  0  0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 0  0  0]\n",
      "  [ 0  0  0]\n",
      "  [ 0  0  0]\n",
      "  ...\n",
      "  [ 0  0  0]\n",
      "  [ 0  0  0]\n",
      "  [ 0  0  0]]\n",
      "\n",
      " [[ 0  0  0]\n",
      "  [ 0  0  0]\n",
      "  [ 0  0  0]\n",
      "  ...\n",
      "  [ 0  0  0]\n",
      "  [ 0  0  0]\n",
      "  [ 0  0  0]]\n",
      "\n",
      " [[ 0  0  0]\n",
      "  [ 0  0  0]\n",
      "  [ 0  0  0]\n",
      "  ...\n",
      "  [ 0  0  0]\n",
      "  [ 0  0  0]\n",
      "  [ 0  0  0]]]\n",
      "60 \n",
      " [[[165 126 116]\n",
      "  [152 111 101]\n",
      "  [166 122 113]\n",
      "  ...\n",
      "  [ 38  37  42]\n",
      "  [ 29  27  33]\n",
      "  [ 31  30  36]]\n",
      "\n",
      " [[164 124 114]\n",
      "  [146 104  95]\n",
      "  [169 126 117]\n",
      "  ...\n",
      "  [ 37  36  39]\n",
      "  [ 41  40  44]\n",
      "  [ 41  41  44]]\n",
      "\n",
      " [[169 130 120]\n",
      "  [161 120 111]\n",
      "  [164 121 112]\n",
      "  ...\n",
      "  [ 41  41  41]\n",
      "  [ 44  44  44]\n",
      "  [ 49  49  49]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 18  20  24]\n",
      "  [ 17  19  23]\n",
      "  [ 14  16  20]\n",
      "  ...\n",
      "  [ 18  25  29]\n",
      "  [ 23  25  29]\n",
      "  [ 27  24  29]]\n",
      "\n",
      " [[ 20  21  26]\n",
      "  [ 17  18  23]\n",
      "  [ 17  18  23]\n",
      "  ...\n",
      "  [ 17  27  28]\n",
      "  [ 20  25  27]\n",
      "  [ 26  24  29]]\n",
      "\n",
      " [[ 19  20  25]\n",
      "  [ 21  22  27]\n",
      "  [ 19  20  25]\n",
      "  ...\n",
      "  [ 16  26  27]\n",
      "  [ 21  25  28]\n",
      "  [ 28  27  32]]]\n",
      "ok, 334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adam/opt/anaconda3/lib/python3.8/site-packages/numpy/core/_asarray.py:136: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order, subok=True)\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os, glob\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 분류 대상 카테고리 선택하기 --- (※1)\n",
    "caltech_dir = \"./data/image\"\n",
    "categories = [\"chair\",\"camera\",\"butterfly\",\"elephant\",\"flamingo\"]\n",
    "nb_classes = len(categories)\n",
    "\n",
    "# 이미지 크기 지정 --- (※2)\n",
    "image_w = 64 \n",
    "image_h = 64\n",
    "pixels = image_w * image_h * 3\n",
    "\n",
    "# 이미지 데이터 읽어 들이기 --- (※3)\n",
    "X = []\n",
    "Y = []\n",
    "for idx, cat in enumerate(categories):\n",
    "    # 레이블 지정 --- (※4)\n",
    "    label = [0 for i in range(nb_classes)]\n",
    "    label[idx] = 1\n",
    "    # 이미지 --- (※5)\n",
    "    image_dir = caltech_dir + \"/\" + cat\n",
    "    files = glob.glob(image_dir+\"/*.jpg\")\n",
    "    for i, f in enumerate(files):\n",
    "        img = Image.open(f) # --- (※6)\n",
    "        img = img.convert(\"RGB\")\n",
    "        img = img.resize((image_w, image_h))\n",
    "        data = np.asarray(img)\n",
    "        X.append(data)\n",
    "        Y.append(label)\n",
    "        if i % 10 == 0:\n",
    "            print(i, \"\\n\", data)\n",
    "X = np.array(X)\n",
    "Y = np.array(Y)\n",
    "\n",
    "# 학습 전용 데이터와 테스트 전용 데이터 구분 --- (※7)\n",
    "X_train, X_test, y_train, y_test = \\\n",
    "    train_test_split(X, Y)\n",
    "xy = (X_train, X_test, y_train, y_test)\n",
    "np.save(\"./data/image/5obj.npy\", xy)\n",
    "print(\"ok,\", len(Y))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b669e7ac",
   "metadata": {},
   "source": [
    "#### CNN 으로 이미지 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c965693f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (250, 64, 64, 3)\n",
      "(64, 64, 3)\n",
      "(84, 64, 64, 3)\n",
      "WARNING:tensorflow:5 out of the last 13 calls to <function Model.make_test_function.<locals>.test_function at 0x7fb8a281d670> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "3/3 [==============================] - 1s 16ms/step - loss: 0.5382 - accuracy: 0.7206\n",
      "loss= 0.489135205745697\n",
      "accuracy= 0.738095223903656\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "import numpy as np\n",
    "\n",
    "# 카테고리 지정하기\n",
    "categories = [\"chair\",\"camera\",\"butterfly\",\"elephant\",\"flamingo\"]\n",
    "nb_classes = len(categories)\n",
    "\n",
    "# 이미지 크기 지정하기\n",
    "image_w = 64 \n",
    "image_h = 64\n",
    "\n",
    "# 데이터 불러오기 --- (※1)\n",
    "X_train, X_test, y_train, y_test = np.load(\"./data/image/5obj.npy\", allow_pickle=True)\n",
    "# 데이터 정규화하기\n",
    "X_train = X_train.astype(\"float\") / 256\n",
    "X_test  = X_test.astype(\"float\")  / 256\n",
    "print('X_train shape:', X_train.shape)\n",
    "\n",
    "# 모델 구축하기 --- (※2)\n",
    "model = Sequential()\n",
    "\n",
    "print(X_train.shape[1:])\n",
    "model.add(Convolution2D(32, kernel_size=3, padding='same',input_shape=(64,64,3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(2))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "\n",
    "model.add(Convolution2D(64, kernel_size=3, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Convolution2D(64, 3, 3))\n",
    "model.add(MaxPooling2D(2))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten()) # --- (※3) \n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "    optimizer='rmsprop',\n",
    "    metrics=['accuracy'])\n",
    "\n",
    "# 모델 훈련하기 --- (※4)\n",
    "hdf5_file = \"./data/image/5obj-model.hdf5\"\n",
    "if os.path.exists(hdf5_file):\n",
    "    model.load_weights(hdf5_file)\n",
    "else:\n",
    "    model.fit(X_train, y_train, batch_size=32, epochs=50)\n",
    "    model.save_weights(hdf5_file)\n",
    "    \n",
    "# 모델 평가하기--- (※5)\n",
    "print(X_test.shape)\n",
    "score = model.evaluate(X_test, y_test)\n",
    "print('loss=', score[0])\n",
    "print('accuracy=', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360a280b",
   "metadata": {},
   "source": [
    "### 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6e8872c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.9988806e-01 2.7999952e-06 9.4584817e-07 4.5652785e-14 1.0827886e-04]]\n",
      "0 ('chair', 0.99988806)\n",
      "1 ('camera', 2.7999952e-06)\n",
      "2 ('butterfly', 9.4584817e-07)\n",
      "3 ('elephant', 4.5652785e-14)\n",
      "4 ('flamingo', 0.00010827886)\n"
     ]
    }
   ],
   "source": [
    "img = Image.open('./data/image/chair.png')\n",
    "img = img.convert(\"RGB\")\n",
    "img = img.resize((image_w, image_h))\n",
    "data = np.asarray(img)\n",
    "data = data.astype(\"float\") / 256\n",
    "data = data.reshape(-1, 64, 64, 3)\n",
    "score = model.predict(data)\n",
    "print(score)\n",
    "for category, prob in enumerate(zip(categories, score[0])):\n",
    "    print(category, prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60625642",
   "metadata": {},
   "source": [
    "### 모델 평가하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "fdcf887e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NG] elephant != chair\n",
      "[0. 0. 0. 1. 0.]\n",
      "[NG] chair != butterfly\n",
      "[9.9992096e-01 8.3999779e-21 8.8200629e-09 7.8986428e-05 2.5187868e-12]\n",
      "[NG] elephant != chair\n",
      "[1.8270513e-02 1.5842220e-06 5.4390505e-02 9.2733598e-01 1.4643543e-06]\n",
      "[NG] chair != elephant\n",
      "[9.8502654e-01 1.1607931e-04 4.1011488e-05 5.7809000e-05 1.4758540e-02]\n",
      "[NG] elephant != camera\n",
      "[2.9985080e-04 4.5807310e-03 4.4517943e-01 5.4993993e-01 1.7782527e-09]\n",
      "[NG] flamingo != chair\n",
      "[1.3649499e-01 7.9987700e-08 1.8803734e-09 3.1283756e-15 8.6350489e-01]\n",
      "[NG] elephant != flamingo\n",
      "[2.4393796e-04 9.6005097e-02 3.5548590e-03 8.9547420e-01 4.7219186e-03]\n",
      "[NG] butterfly != elephant\n",
      "[4.4447283e-04 2.8319203e-06 9.9693918e-01 4.2618372e-05 2.5708063e-03]\n",
      "[NG] butterfly != flamingo\n",
      "[2.2352284e-09 8.6503330e-20 1.0000000e+00 3.0703858e-15 3.6942367e-08]\n",
      "[NG] chair != flamingo\n",
      "[9.8785758e-01 9.5411912e-03 2.3005588e-03 1.2744877e-07 3.0049484e-04]\n",
      "[NG] butterfly != camera\n",
      "[9.6826284e-07 3.9146366e-03 9.7874272e-01 4.7780798e-08 1.7341681e-02]\n",
      "[NG] elephant != chair\n",
      "[3.4040830e-04 1.0819545e-05 1.1421527e-09 9.9893445e-01 7.1437721e-04]\n",
      "[NG] flamingo != butterfly\n",
      "[1.3347134e-06 5.9128662e-13 1.2501066e-04 7.6857520e-13 9.9987364e-01]\n",
      "[NG] chair != flamingo\n",
      "[4.8288006e-01 4.1997130e-09 4.9064454e-02 6.0310965e-11 4.6805555e-01]\n",
      "[NG] chair != camera\n",
      "[7.5455379e-01 2.5024209e-02 2.1956563e-01 2.3060427e-06 8.5395627e-04]\n",
      "[NG] flamingo != butterfly\n",
      "[1.1529642e-04 7.0299009e-08 1.3038749e-01 5.3986241e-06 8.6949176e-01]\n",
      "[NG] camera != elephant\n",
      "[2.8800914e-06 6.9637364e-01 1.5962109e-05 1.1291799e-01 1.9068955e-01]\n",
      "[NG] flamingo != elephant\n",
      "[6.2548099e-03 1.8555349e-10 4.2200316e-08 1.0378822e-08 9.9374515e-01]\n",
      "[NG] elephant != butterfly\n",
      "[1.0210637e-03 9.5286023e-06 1.0423115e-06 9.9890673e-01 6.1718587e-05]\n",
      "[NG] elephant != chair\n",
      "[1.4847250e-01 2.7601671e-11 3.1969183e-08 8.5152650e-01 9.7784437e-07]\n",
      "[NG] elephant != chair\n",
      "[1.5452393e-02 2.5536147e-07 1.5088325e-09 9.8454720e-01 1.1702187e-07]\n",
      "[NG] elephant != chair\n",
      "[8.3060062e-05 1.1581436e-05 1.1355454e-03 7.9268265e-01 2.0608708e-01]\n"
     ]
    }
   ],
   "source": [
    "# 모델 평가하기\n",
    "pre = model.predict(X_test)\n",
    "for i,v in enumerate(pre):\n",
    "    pre_ans = v.argmax()\n",
    "    ans = y_test[i].argmax()\n",
    "    dat = X_test[i]\n",
    "    if ans == pre_ans: continue\n",
    "    print(\"[NG]\", categories[pre_ans], \"!=\", categories[ans])\n",
    "    print(v)\n",
    "    fname = \"./data/image/error/\" + str(i) + \"-\" + categories[pre_ans] + \\\n",
    "        \"-ne-\" + categories[ans] + \".png\"\n",
    "    dat *= 256\n",
    "    img = Image.fromarray(np.uint8(dat))\n",
    "    img.save(fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d494066f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83cf7baa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06bb3db4",
   "metadata": {},
   "source": [
    "## Object Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6fdfed",
   "metadata": {},
   "source": [
    "### 이미지에서 얼굴 검출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c8d1d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ref: https://docs.opencv.org/3.4.1/d7/d8b/tutorial_py_face_detection.html\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "faceCascade= cv2.CascadeClassifier(\n",
    "      './haarcascades/haarcascade_frontalface_default.xml')\n",
    "eyeCascade = cv2.CascadeClassifier(\n",
    "    './haarcascades/haarcascade_eye.xml')\n",
    "\n",
    "src = cv2.imread('./data/lena.jpg')\n",
    "gray = cv2.cvtColor(src, cv2.COLOR_BGR2GRAY)\n",
    "faces = faceCascade.detectMultiScale(gray, 1.1, 3) #(gray, 1.1, 0)\n",
    "\n",
    "for (x, y, w, h) in faces:\n",
    "    cv2.rectangle(src, (x,y),(x+w, y+h),(255,0,0), 2)\n",
    "    \n",
    "    roi_gray  = gray[y:y+h, x:x+w]\n",
    "    roi_color = src[y:y+h, x:x+w]\n",
    "    \n",
    "    eyes = eyeCascade.detectMultiScale(roi_gray)\n",
    "    for (ex,ey,ew,eh) in eyes:\n",
    "        cv2.rectangle(roi_color,(ex,ey),(ex+ew,ey+eh),(0,255,0),2)\n",
    "        \n",
    "cv2.imshow('src', src)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0dbf60",
   "metadata": {},
   "source": [
    "### 유투브 동영상에서 검출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a650c31e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title =  참이슬 아이유 X 박서준 바이럴영상(30\")\n",
      "best.resolution 1280x720\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    " pip install youtube_dl\n",
    " pip install pafy\n",
    "'''\n",
    "import numpy as np\n",
    "import cv2, pafy\n",
    "\n",
    "faceCascade= cv2.CascadeClassifier(\n",
    "      './haarcascades/haarcascade_frontalface_default.xml')\n",
    "\n",
    "url = 'https://www.youtube.com/watch?v=S_0ikqqccJs'\n",
    "video = pafy.new(url)\n",
    "print('title = ', video.title)\n",
    "\n",
    "best = video.getbest(preftype='mp4')\n",
    "print('best.resolution', best.resolution)\n",
    "\n",
    "cap=cv2.VideoCapture(best.url)\n",
    "while(True):\n",
    "        retval, frame = cap.read()\n",
    "        if not retval:\n",
    "                break\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        faces = faceCascade.detectMultiScale(gray) \n",
    "        #minSize=(50, 50)\n",
    "        for (x, y, w, h) in faces:\n",
    "            cv2.rectangle(frame, (x,y),(x+w, y+h),(255,0,0), 2)           \n",
    "        cv2.imshow('frame',frame)\n",
    " \n",
    "        key = cv2.waitKey(25)\n",
    "        if key == 27: # Esc\n",
    "                break\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74f8901",
   "metadata": {},
   "source": [
    "### 얼굴 모자이크 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f0f580f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/photo2-mosaic.jpg\n",
      "[[599 481 205 205]\n",
      " [293 142 239 239]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cv2, sys, re\n",
    "# 입력 파일 지정하기 --- (※1)\n",
    "image_file = './data/photo2.jpg'\n",
    "\n",
    "# 출력 파일 이름\n",
    "output_file = re.sub(r'\\.jpg|jpeg|PNG$', '-mosaic.jpg', image_file)\n",
    "print(output_file)\n",
    "mosaic_rate = 30 \n",
    "\n",
    "# 캐스캐이드 파일 경로 지정하기\n",
    "cascade_file = cv2.data.haarcascades + \"haarcascade_frontalface_alt.xml\"\n",
    "\n",
    "# 이미지 읽어 들이기 --- (※2)\n",
    "image = cv2.imread(image_file)\n",
    "image_gs = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) # 그레이스케일 변환\n",
    "# 얼굴 인식 실행하기 --- (※3)\n",
    "cascade = cv2.CascadeClassifier(cascade_file)\n",
    "face_list = cascade.detectMultiScale(image_gs,\n",
    "    scaleFactor=1.1,\n",
    "    minNeighbors=1,\n",
    "    minSize=(100,100))\n",
    "if len(face_list) == 0:\n",
    "    print(\"no face\")\n",
    "    quit()\n",
    "# 확인한 부분에 모자이크 걸기 -- (※4)\n",
    "print(face_list)\n",
    "color = (0, 0, 255)\n",
    "for (x,y,w,h) in face_list:\n",
    "    # 얼굴 부분 자르기 --- (※5)\n",
    "    face_img = image[y:y+h, x:x+w]\n",
    "    # 자른 이미지를 지정한 배율로 확대/축소하기 --- (※6)\n",
    "    face_img = cv2.resize(face_img, (w//mosaic_rate, h//mosaic_rate))\n",
    "    # 확대/축소한 그림을 원래 크기로 돌리기 --- (※7)\n",
    "    face_img = cv2.resize(face_img, (w, h), \n",
    "        interpolation=cv2.INTER_AREA)\n",
    "    # 원래 이미지에 붙이기 --- (※8)\n",
    "    image[y:y+h, x:x+w] = face_img\n",
    "# 렌더링 결과를 파일에 출력\n",
    "cv2.imwrite(output_file, image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7463bbc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e98726b",
   "metadata": {},
   "source": [
    "## 얼굴 이미지 분류 인식"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f97c743",
   "metadata": {},
   "source": [
    "### AT&T 얼굴 데이터베이스 얼굴 인식 - EigenFaceRecognizer, FisherFaceRecognizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "362007bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_faces.shape= (320, 10304)\n",
      "train_labels.shape= (320,)\n",
      "test_faces.shape= (80, 10304)\n",
      "test_labels.shape= (80,)\n",
      "eigenFace.shape= (320, 10304)\n",
      "test_labels=20: predicted:20, confidence=2014.5525962455326\n",
      "test_labels=15: predicted:15, confidence=1121.1903491079265\n",
      "test_labels=26: predicted:26, confidence=1274.7274938741857\n",
      "test_labels=33: predicted:33, confidence=3071.153795486296\n",
      "test_labels=38: predicted:38, confidence=2470.4316800716247\n",
      "test_labels=21: predicted:21, confidence=2678.164527128312\n",
      "test_labels=32: predicted:32, confidence=2509.358882871588\n",
      "test_labels=34: predicted:34, confidence=2076.6811249837956\n",
      "test_labels=0: predicted:0, confidence=3471.8626540013893\n",
      "test_labels=28: predicted:28, confidence=2122.3041822479827\n",
      "test_labels=5: predicted:5, confidence=2563.997763461227\n",
      "test_labels=4: predicted:4, confidence=1334.252467895507\n",
      "test_labels=3: predicted:3, confidence=2274.3981298760664\n",
      "test_labels=28: predicted:28, confidence=2910.0390937160023\n",
      "test_labels=22: predicted:22, confidence=1604.6933980372255\n",
      "test_labels=19: predicted:19, confidence=2950.229130454185\n",
      "test_labels=23: predicted:23, confidence=2353.636269073039\n",
      "test_labels=30: predicted:30, confidence=2601.4567825575064\n",
      "test_labels=10: predicted:10, confidence=2579.3544859879635\n",
      "test_labels=21: predicted:21, confidence=1975.2800700561634\n",
      "test_labels=6: predicted:6, confidence=1742.8866868257294\n",
      "test_labels=32: predicted:32, confidence=2037.1073653565293\n",
      "test_labels=38: predicted:38, confidence=1788.8291673410486\n",
      "test_labels=2: predicted:2, confidence=2608.5100719822317\n",
      "test_labels=9: predicted:9, confidence=1755.4218136995285\n",
      "test_labels=16: predicted:16, confidence=1886.4432251990293\n",
      "test_labels=31: predicted:31, confidence=2329.157092832742\n",
      "test_labels=38: predicted:38, confidence=1471.5306742380596\n",
      "test_labels=35: predicted:35, confidence=1251.9341442667162\n",
      "test_labels=10: predicted:10, confidence=1435.0718199373243\n",
      "test_labels=3: predicted:3, confidence=2528.913054611389\n",
      "test_labels=37: predicted:37, confidence=1957.1882835316146\n",
      "test_labels=2: predicted:2, confidence=2504.87964790676\n",
      "test_labels=9: predicted:9, confidence=1860.3110387868344\n",
      "test_labels=8: predicted:8, confidence=2487.0349219381146\n",
      "test_labels=30: predicted:30, confidence=2792.500605470204\n",
      "test_labels=34: predicted:34, confidence=3086.0328130002417\n",
      "test_labels=8: predicted:8, confidence=2095.8921914289986\n",
      "test_labels=12: predicted:12, confidence=2123.9421270127827\n",
      "test_labels=26: predicted:26, confidence=2861.079470063701\n",
      "test_labels=22: predicted:22, confidence=2341.750280183978\n",
      "test_labels=33: predicted:33, confidence=1600.8602926095537\n",
      "test_labels=34: predicted:35, confidence=2847.9709292997177\n",
      "test_labels=23: predicted:23, confidence=1980.7155473433822\n",
      "test_labels=13: predicted:13, confidence=1713.3726992378283\n",
      "test_labels=30: predicted:30, confidence=1985.0089127805952\n",
      "test_labels=25: predicted:25, confidence=2079.89646419533\n",
      "test_labels=26: predicted:26, confidence=2019.8996404739337\n",
      "test_labels=37: predicted:37, confidence=2567.002238557754\n",
      "test_labels=30: predicted:30, confidence=1983.6084873475284\n",
      "test_labels=39: predicted:39, confidence=2640.8870711453387\n",
      "test_labels=28: predicted:28, confidence=3740.8179528624646\n",
      "test_labels=5: predicted:5, confidence=2266.4792095535527\n",
      "test_labels=35: predicted:35, confidence=1688.6228236931984\n",
      "test_labels=18: predicted:18, confidence=1408.2227289843993\n",
      "test_labels=15: predicted:15, confidence=1879.3179083180516\n",
      "test_labels=25: predicted:25, confidence=2177.781436373055\n",
      "test_labels=39: predicted:39, confidence=2129.009738633883\n",
      "test_labels=10: predicted:10, confidence=1935.446091106256\n",
      "test_labels=3: predicted:3, confidence=2826.8207610871905\n",
      "test_labels=26: predicted:26, confidence=2304.4168003267255\n",
      "test_labels=8: predicted:8, confidence=1862.5667688751137\n",
      "test_labels=8: predicted:8, confidence=841.598505178181\n",
      "test_labels=8: predicted:8, confidence=1772.8305084938197\n",
      "test_labels=12: predicted:12, confidence=1855.83336520829\n",
      "test_labels=3: predicted:3, confidence=2114.9861744800583\n",
      "test_labels=5: predicted:5, confidence=2095.682518651708\n",
      "test_labels=17: predicted:17, confidence=2158.696880142175\n",
      "test_labels=34: predicted:34, confidence=2116.1262208996336\n",
      "test_labels=35: predicted:35, confidence=1781.498561756365\n",
      "test_labels=28: predicted:28, confidence=3267.6898963409403\n",
      "test_labels=11: predicted:11, confidence=1933.4237562792878\n",
      "test_labels=38: predicted:38, confidence=1564.9865639450024\n",
      "test_labels=15: predicted:15, confidence=1523.8150142720442\n",
      "test_labels=7: predicted:7, confidence=2373.065930786771\n",
      "test_labels=19: predicted:19, confidence=1586.8073880911127\n",
      "test_labels=27: predicted:27, confidence=822.6686847742621\n",
      "test_labels=18: predicted:18, confidence=2413.5526297786464\n",
      "test_labels=34: predicted:34, confidence=2478.1191414894524\n",
      "test_labels=10: predicted:10, confidence=1020.3260898884128\n",
      "accuracy= 0.9875\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "#1\n",
    "WIDTH = 92\n",
    "HEIGHT = 112\n",
    "def load_face(filename='./data/faces.csv', test_ratio=0.2):\n",
    "    file = open(filename, 'r')\n",
    "    lines = file.readlines()\n",
    "\n",
    "    N = len(lines)\n",
    "    faces = np.empty((N, WIDTH*HEIGHT), dtype=np.uint8 )\n",
    "    labels = np.empty(N, dtype = np.int32)\n",
    "    for i, line in enumerate(lines):\n",
    "        filename, label = line.strip().split(';')\n",
    "        labels[i] = int(label)\n",
    "        img = cv2.imread(filename, cv2.IMREAD_GRAYSCALE)\n",
    "        faces[i, :] = img.flatten()\n",
    "  \n",
    "# shuffling and seperate train and test data\n",
    "    indices = list(range(N))\n",
    "    random.seed(1) # same random sequences, so the same result\n",
    "    random.shuffle(indices)\n",
    "    shuffle_faces = faces[indices]\n",
    "    shuffle_labels = labels[indices]\n",
    "\n",
    "    test_size = int(test_ratio*N)\n",
    "\n",
    "    test_faces = shuffle_faces[:test_size]\n",
    "    test_labels = shuffle_labels[:test_size]\n",
    "\n",
    "    train_faces = shuffle_faces[test_size:]\n",
    "    train_labels = shuffle_labels[test_size:]\n",
    "    return train_faces, train_labels, test_faces, test_labels\n",
    "\n",
    "#2\n",
    "train_faces, train_labels, test_faces, test_labels = load_face()\n",
    "print('train_faces.shape=',  train_faces.shape)\n",
    "print('train_labels.shape=', train_labels.shape)\n",
    "print('test_faces.shape=',   test_faces.shape)\n",
    "print('test_labels.shape=',  test_labels.shape)\n",
    "\n",
    "#3    \n",
    "recognizer = cv2.face.EigenFaceRecognizer_create()\n",
    "##recognizer = cv2.face.FisherFaceRecognizer_create()\n",
    "recognizer.train(train_faces.reshape(-1, HEIGHT, WIDTH), train_labels)\n",
    "\n",
    "#4: display eigen Face\n",
    "eigenFace = recognizer.getEigenVectors()\n",
    "eigenFace = eigenFace.T\n",
    "print('eigenFace.shape=',  eigenFace.shape)\n",
    "\n",
    "dst = np.zeros((8*HEIGHT, 10*WIDTH), dtype=np.uint8)\n",
    "\n",
    "##for i in range(39): # FisherFaceRecognizer\n",
    "for i in range(80):\n",
    "  x = i%10\n",
    "  y = i//10\n",
    "  x1 = x*WIDTH\n",
    "  y1 = y*HEIGHT\n",
    "  x2 = x1+WIDTH\n",
    "  y2 = y1+HEIGHT  \n",
    "  \n",
    "  img = eigenFace[i].reshape(HEIGHT, WIDTH)\n",
    "  dst[y1:y2, x1:x2] = cv2.normalize(img,None,0,255,cv2.NORM_MINMAX)\n",
    "cv2.imshow('eigenFace 80', dst)\n",
    "\n",
    "#5: predict test_faces using recognizer\n",
    "correct_count = 0\n",
    "for i, face in enumerate(test_faces): \n",
    "    predict_label, confidence = recognizer.predict(face)\n",
    "    if test_labels[i]== predict_label:\n",
    "        correct_count+= 1\n",
    "    print('test_labels={}: predicted:{}, confidence={}'.format(\n",
    "                     test_labels[i], predict_label, confidence))\n",
    "accuracy = correct_count / float(len(test_faces))\n",
    "print('accuracy=', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d50f55",
   "metadata": {},
   "source": [
    "### AT&T 얼굴 데이터베이스를 이용한 얼굴 인식 : LBPHFaceRecognizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dcc3559a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_faces.shape= (320, 10304)\n",
      "train_labels.shape= (320,)\n",
      "test_faces.shape= (80, 10304)\n",
      "test_labels.shape= (80,)\n",
      "test_labels=20: predicted:20, confidence=63.53159239674068\n",
      "test_labels=15: predicted:15, confidence=56.76836690095442\n",
      "test_labels=26: predicted:26, confidence=65.35428328380661\n",
      "test_labels=33: predicted:33, confidence=78.94946641055041\n",
      "test_labels=38: predicted:38, confidence=75.97988206043449\n",
      "test_labels=21: predicted:21, confidence=82.08887829286003\n",
      "test_labels=32: predicted:32, confidence=84.18183984479495\n",
      "test_labels=34: predicted:34, confidence=72.16901863930305\n",
      "test_labels=0: predicted:0, confidence=78.75554190227402\n",
      "test_labels=28: predicted:28, confidence=63.16955227973658\n",
      "test_labels=5: predicted:5, confidence=69.28946916637393\n",
      "test_labels=4: predicted:4, confidence=64.63933640341054\n",
      "test_labels=3: predicted:3, confidence=66.44944519613094\n",
      "test_labels=28: predicted:28, confidence=73.32473172587548\n",
      "test_labels=22: predicted:22, confidence=61.721612437042324\n",
      "test_labels=19: predicted:19, confidence=72.6869564763709\n",
      "test_labels=23: predicted:23, confidence=70.08252144563349\n",
      "test_labels=30: predicted:30, confidence=73.68417603002699\n",
      "test_labels=10: predicted:10, confidence=64.87692069784792\n",
      "test_labels=21: predicted:21, confidence=68.60931018355592\n",
      "test_labels=6: predicted:6, confidence=62.42943779654621\n",
      "test_labels=32: predicted:32, confidence=72.38653452106406\n",
      "test_labels=38: predicted:38, confidence=56.316642002959085\n",
      "test_labels=2: predicted:2, confidence=68.48076569718461\n",
      "test_labels=9: predicted:9, confidence=68.16472800570462\n",
      "test_labels=16: predicted:16, confidence=68.61633816903462\n",
      "test_labels=31: predicted:31, confidence=69.23613385192938\n",
      "test_labels=38: predicted:38, confidence=58.249879241662676\n",
      "test_labels=35: predicted:35, confidence=63.59130641274919\n",
      "test_labels=10: predicted:10, confidence=57.28739592441953\n",
      "test_labels=3: predicted:3, confidence=66.29421647663028\n",
      "test_labels=37: predicted:37, confidence=64.80227136289078\n",
      "test_labels=2: predicted:2, confidence=67.04763300158947\n",
      "test_labels=9: predicted:9, confidence=70.57010266230759\n",
      "test_labels=8: predicted:8, confidence=67.25114029813888\n",
      "test_labels=30: predicted:30, confidence=69.76173278447251\n",
      "test_labels=34: predicted:13, confidence=87.57682723101284\n",
      "test_labels=8: predicted:8, confidence=71.04794581172273\n",
      "test_labels=12: predicted:12, confidence=61.47554788049576\n",
      "test_labels=26: predicted:26, confidence=78.54355391725223\n",
      "test_labels=22: predicted:22, confidence=73.91948275316803\n",
      "test_labels=33: predicted:33, confidence=62.47540709037265\n",
      "test_labels=34: predicted:35, confidence=86.52634471217993\n",
      "test_labels=23: predicted:23, confidence=67.77178986859664\n",
      "test_labels=13: predicted:13, confidence=70.04312613113598\n",
      "test_labels=30: predicted:30, confidence=66.81501615748402\n",
      "test_labels=25: predicted:25, confidence=64.21791860822599\n",
      "test_labels=26: predicted:26, confidence=72.50338916802869\n",
      "test_labels=37: predicted:37, confidence=69.99859890707056\n",
      "test_labels=30: predicted:30, confidence=64.63257168302847\n",
      "test_labels=39: predicted:39, confidence=73.21721325539515\n",
      "test_labels=28: predicted:28, confidence=87.95501110026858\n",
      "test_labels=5: predicted:5, confidence=67.40362084192925\n",
      "test_labels=35: predicted:35, confidence=65.28257013254459\n",
      "test_labels=18: predicted:18, confidence=68.15449331395266\n",
      "test_labels=15: predicted:15, confidence=67.6856704732354\n",
      "test_labels=25: predicted:25, confidence=66.1634956069417\n",
      "test_labels=39: predicted:39, confidence=66.6242563542645\n",
      "test_labels=10: predicted:10, confidence=60.68353110930176\n",
      "test_labels=3: predicted:3, confidence=68.81583016512818\n",
      "test_labels=26: predicted:26, confidence=71.17717358932501\n",
      "test_labels=8: predicted:8, confidence=68.15621119764268\n",
      "test_labels=8: predicted:8, confidence=58.33452946028829\n",
      "test_labels=8: predicted:8, confidence=61.24375663336904\n",
      "test_labels=12: predicted:12, confidence=67.64116227275328\n",
      "test_labels=3: predicted:3, confidence=65.24171492107168\n",
      "test_labels=5: predicted:5, confidence=63.04036885963557\n",
      "test_labels=17: predicted:17, confidence=70.51412910520007\n",
      "test_labels=34: predicted:34, confidence=73.36626728128859\n",
      "test_labels=35: predicted:35, confidence=72.71897099216419\n",
      "test_labels=28: predicted:28, confidence=80.63635251331333\n",
      "test_labels=11: predicted:11, confidence=61.85779561147603\n",
      "test_labels=38: predicted:38, confidence=57.0722691349092\n",
      "test_labels=15: predicted:15, confidence=67.59070210956943\n",
      "test_labels=7: predicted:7, confidence=67.61870739215279\n",
      "test_labels=19: predicted:19, confidence=60.2657618601104\n",
      "test_labels=27: predicted:27, confidence=60.327109180596864\n",
      "test_labels=18: predicted:18, confidence=74.12660136323477\n",
      "test_labels=34: predicted:34, confidence=80.59929453523937\n",
      "test_labels=10: predicted:10, confidence=56.43321379562106\n",
      "accuracy= 0.975\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "#1\n",
    "WIDTH = 92\n",
    "HEIGHT = 112\n",
    "def load_face(filename='./data/faces.csv', test_ratio=0.2):\n",
    "    file = open(filename, 'r')\n",
    "    lines = file.readlines()\n",
    "\n",
    "    N = len(lines)\n",
    "    faces = np.empty((N, WIDTH*HEIGHT), dtype=np.uint8 )\n",
    "    labels = np.empty(N, dtype = np.int32)\n",
    "    for i, line in enumerate(lines):\n",
    "        filename, label = line.strip().split(';')\n",
    "        labels[i] = int(label)\n",
    "        img = cv2.imread(filename, cv2.IMREAD_GRAYSCALE)\n",
    "        faces[i, :] = img.flatten()\n",
    "  \n",
    "# shuffling and seperate train and test data\n",
    "    indices = list(range(N))\n",
    "    random.seed(1) # same random sequences, so the same result\n",
    "    random.shuffle(indices)\n",
    "    shuffle_faces = faces[indices]\n",
    "    shuffle_labels = labels[indices]\n",
    "\n",
    "    test_size = int(test_ratio*N)\n",
    "\n",
    "    test_faces = shuffle_faces[:test_size]\n",
    "    test_labels = shuffle_labels[:test_size]\n",
    "\n",
    "    train_faces = shuffle_faces[test_size:]\n",
    "    train_labels = shuffle_labels[test_size:]\n",
    "    return train_faces, train_labels, test_faces, test_labels\n",
    "\n",
    "train_faces, train_labels, test_faces, test_labels = load_face()\n",
    "print('train_faces.shape=',  train_faces.shape)\n",
    "print('train_labels.shape=', train_labels.shape)\n",
    "print('test_faces.shape=',   test_faces.shape)\n",
    "print('test_labels.shape=',  test_labels.shape)\n",
    "\n",
    "#2    \n",
    "recognizer = cv2.face.LBPHFaceRecognizer_create()\n",
    "recognizer.train(train_faces.reshape(-1, HEIGHT, WIDTH), train_labels)\n",
    " \n",
    "#3: predict test_faces using recognizer\n",
    "correct_count = 0\n",
    "for i, face in enumerate(test_faces.reshape(-1, HEIGHT, WIDTH)):    \n",
    "    predict_label, confidence = recognizer.predict(face)\n",
    "    if test_labels[i]== predict_label:\n",
    "        correct_count+= 1\n",
    "    print('test_labels={}: predicted:{}, confidence={}'.format(\n",
    "                     test_labels[i], predict_label,confidence))\n",
    "accuracy = correct_count / float(len(test_faces))\n",
    "print('accuracy=', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0b7840",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "480ce761",
   "metadata": {},
   "source": [
    "## 지식 증류"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c2a175",
   "metadata": {},
   "source": [
    "### 필요한 라이브러리 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f00912d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import cv2\n",
    "#from google.colab.patches import cv2_imshow\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e881767",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 파라미터 설정\n",
    "t_ephoc = 10 #@param {type:\"slider\", min:1, max:100, step:1}\n",
    "s_ephoc = 5 #@param {type:\"slider\", min:1, max:100, step:1}\n",
    "learning_rate = 0.01 \n",
    "batch_size = 64 #@param [32, 64, 128, 256] {type:\"raw\"}\n",
    "temperature = 3 #@param {type:\"slider\", min:1, max:10, step:1}\n",
    "alpha = 0.5 #@param {type:\"slider\", min:0.1, max:0.9, step:0.1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d2d2924",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MNIST 데이터셋 가져오기\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype(\"float32\") / 255.0\n",
    "x_train = np.reshape(x_train, (-1, 28, 28, 1))\n",
    "\n",
    "x_test = x_test.astype(\"float32\") / 255.0\n",
    "x_test = np.reshape(x_test, (-1, 28, 28, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "973d743a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 14, 14, 256)       2560      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 14, 14, 256)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 14, 14, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 7, 7, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10)                250890    \n",
      "=================================================================\n",
      "Total params: 1,433,610\n",
      "Trainable params: 1,433,610\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-05 14:02:01.019759: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# 파라미터가 많은 모델\n",
    "i=tf.keras.Input(shape=(28, 28, 1))\n",
    "out=tf.keras.layers.Conv2D(256, (3, 3), strides=(2, 2), padding=\"same\")(i)\n",
    "out=tf.keras.layers.LeakyReLU(alpha=0.2)(out)\n",
    "out=tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\")(out)\n",
    "out=tf.keras.layers.Conv2D(512, (3, 3), strides=(2, 2), padding=\"same\")(out)\n",
    "out=tf.keras.layers.Flatten()(out)\n",
    "out=tf.keras.layers.Dense(10)(out)\n",
    "t_model=tf.keras.Model(inputs=[i],outputs=[out])\n",
    "\n",
    "t_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "623a4024",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 28, 28, 1)]       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 28)                21980     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                290       \n",
      "=================================================================\n",
      "Total params: 22,270\n",
      "Trainable params: 22,270\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 파라미터 적은 모델\n",
    "i=tf.keras.Input(shape=(28, 28, 1))\n",
    "out=tf.keras.layers.Flatten()(i)\n",
    "out=tf.keras.layers.Dense(28)(out)\n",
    "out=tf.keras.layers.Dense(10)(out)\n",
    "\n",
    "s_model_1=tf.keras.Model(inputs=[i],outputs=[out])\n",
    "s_model_2=tf.keras.models.clone_model(s_model_1)\n",
    "\n",
    "s_model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ec1454fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터가 많은 모델\n",
    "t_model.compile(tf.keras.optimizers.Adam(learning_rate),\n",
    "                tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "# 파라미터가 적은 모델 (distillation 적용)\n",
    "s_model_1.compile(tf.keras.optimizers.Adam(learning_rate),\n",
    "                tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
    "\n",
    "# 비교 모델 (distillation 미적용)\n",
    "s_model_2.compile(tf.keras.optimizers.Adam(learning_rate),\n",
    "                tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "                metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb44c774",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-08-05 14:02:37.763608: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "938/938 [==============================] - 174s 185ms/step - loss: 7.3146 - sparse_categorical_accuracy: 0.9092\n",
      "Epoch 2/10\n",
      "938/938 [==============================] - 170s 181ms/step - loss: 4.4825 - sparse_categorical_accuracy: 0.9559\n",
      "Epoch 3/10\n",
      "938/938 [==============================] - 173s 185ms/step - loss: 6.2008 - sparse_categorical_accuracy: 0.9551\n",
      "Epoch 4/10\n",
      "938/938 [==============================] - 168s 179ms/step - loss: 9.4081 - sparse_categorical_accuracy: 0.9585\n",
      "Epoch 5/10\n",
      "938/938 [==============================] - 169s 180ms/step - loss: 9.4651 - sparse_categorical_accuracy: 0.9644\n",
      "Epoch 6/10\n",
      "938/938 [==============================] - 161s 172ms/step - loss: 10.5941 - sparse_categorical_accuracy: 0.9671\n",
      "Epoch 7/10\n",
      "938/938 [==============================] - 169s 180ms/step - loss: 11.2038 - sparse_categorical_accuracy: 0.9696\n",
      "Epoch 8/10\n",
      "938/938 [==============================] - 182s 194ms/step - loss: 13.5882 - sparse_categorical_accuracy: 0.9698\n",
      "Epoch 9/10\n",
      "938/938 [==============================] - 174s 185ms/step - loss: 11.7473 - sparse_categorical_accuracy: 0.9748\n",
      "Epoch 10/10\n",
      "938/938 [==============================] - 169s 180ms/step - loss: 12.5424 - sparse_categorical_accuracy: 0.9749\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fb71ae199d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 파라미터가 많은 모델 훈련\n",
    "t_model.fit(x_train, y_train,batch_size=batch_size,epochs=t_ephoc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c1525d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파라미터가 적은 모델 손실함수\n",
    "s_loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "# 비교 모델 손실함수\n",
    "d_loss = tf.keras.losses.KLDivergence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "688a7166",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28, 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9523ad0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에포크 0\n",
      "다른 모델을 이용해서 훈련\n",
      "313/313 [==============================] - 0s 632us/step - loss: 0.4862 - sparse_categorical_accuracy: 0.8992\n",
      "단독으로 사용된 경우\n",
      "313/313 [==============================] - 0s 585us/step - loss: 0.3577 - sparse_categorical_accuracy: 0.9018\n",
      "\n",
      "\n",
      "에포크 1\n",
      "다른 모델을 이용해서 훈련\n",
      "313/313 [==============================] - 0s 506us/step - loss: 0.4249 - sparse_categorical_accuracy: 0.9105\n",
      "단독으로 사용된 경우\n",
      "313/313 [==============================] - 0s 503us/step - loss: 0.3281 - sparse_categorical_accuracy: 0.9093\n",
      "\n",
      "\n",
      "에포크 2\n",
      "다른 모델을 이용해서 훈련\n",
      "313/313 [==============================] - 0s 528us/step - loss: 0.4277 - sparse_categorical_accuracy: 0.9101\n",
      "단독으로 사용된 경우\n",
      "313/313 [==============================] - 0s 512us/step - loss: 0.3314 - sparse_categorical_accuracy: 0.9101\n",
      "\n",
      "\n",
      "에포크 3\n",
      "다른 모델을 이용해서 훈련\n",
      "313/313 [==============================] - 0s 496us/step - loss: 0.4146 - sparse_categorical_accuracy: 0.9146\n",
      "단독으로 사용된 경우\n",
      "313/313 [==============================] - 0s 503us/step - loss: 0.3150 - sparse_categorical_accuracy: 0.9144\n",
      "\n",
      "\n",
      "에포크 4\n",
      "다른 모델을 이용해서 훈련\n",
      "313/313 [==============================] - 0s 527us/step - loss: 0.4344 - sparse_categorical_accuracy: 0.9101\n",
      "단독으로 사용된 경우\n",
      "313/313 [==============================] - 0s 550us/step - loss: 0.3377 - sparse_categorical_accuracy: 0.9048\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_count = x_train.shape[0]//batch_size\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(learning_rate)\n",
    "\n",
    "for e in range(s_ephoc):\n",
    "    for _ in range(batch_count):\n",
    "        batch_num=np.random.randint(0, x_train.shape[0], size=batch_size)\n",
    "        t_pred = t_model.predict(x_train[batch_num])\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            s_pred_1 = s_model_1(x_train[batch_num])\n",
    "            student_loss = s_loss(y_train[batch_num], s_pred_1)\n",
    "            distillation_loss = d_loss(\n",
    "                tf.nn.softmax(t_pred / temperature, axis=1),\n",
    "                tf.nn.softmax(s_pred_1 / temperature, axis=1),\n",
    "            )\n",
    "            loss = alpha * student_loss + (1 - alpha) * distillation_loss\n",
    "\n",
    "        vars = s_model_1.trainable_variables\n",
    "        grad = tape.gradient(loss, vars)\n",
    "        opt.apply_gradients(zip(grad, vars))\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            s_pred_2 = s_model_2(x_train[batch_num])\n",
    "            student_loss = s_loss(y_train[batch_num], s_pred_2)\n",
    "        vars = s_model_2.trainable_variables\n",
    "        grad = tape.gradient(student_loss, vars)\n",
    "        opt.apply_gradients(zip(grad, vars))\n",
    "\n",
    "    print(\"에포크 {}\".format(e))\n",
    "    print(\"다른 모델을 이용해서 훈련\")\n",
    "    s_model_1.evaluate(x_test, y_test)\n",
    "    print(\"단독으로 사용된 경우\")\n",
    "    s_model_2.evaluate(x_test, y_test)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc282fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
