{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3907e08f-5ed1-4433-848a-5c5f75df40ff",
   "metadata": {},
   "source": [
    "# Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93707c41-b458-4c73-8846-a1414f530480",
   "metadata": {},
   "source": [
    "## Deep Learning 을 위한 데이터 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1316028c-df65-4c3c-b397-e69c63d549c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using PyTorch version: 1.9.0  Device: cpu\n",
      "X_train: torch.Size([32, 1, 28, 28]) type: torch.FloatTensor\n",
      "y_train: torch.Size([32]) type: torch.LongTensor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adam/opt/anaconda3/lib/python3.8/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  ../torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x72 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "''' 1. Module Import '''\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "''' 2. 딥러닝 모델을 설계할 때 활용하는 장비 확인 '''\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "print('Using PyTorch version:', torch.__version__, ' Device:', DEVICE)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "\n",
    "''' 3. MNIST 데이터 다운로드 (Train set, Test set 분리하기) '''\n",
    "train_dataset = datasets.MNIST(root = \"../data/MNIST\",\n",
    "                               train = True,\n",
    "                               download = True,\n",
    "                               transform = transforms.ToTensor())\n",
    "\n",
    "test_dataset = datasets.MNIST(root = \"../data/MNIST\",\n",
    "                              train = False,\n",
    "                              transform = transforms.ToTensor())\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
    "                                           batch_size = BATCH_SIZE,\n",
    "                                           shuffle = True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
    "                                          batch_size = BATCH_SIZE,\n",
    "                                          shuffle = False)\n",
    "\n",
    "''' 4. 데이터 확인하기 '''\n",
    "for (X_train, y_train) in train_loader:\n",
    "    print('X_train:', X_train.size(), 'type:', X_train.type())\n",
    "    print('y_train:', y_train.size(), 'type:', y_train.type())\n",
    "    break\n",
    "    \n",
    "pltsize = 1\n",
    "plt.figure(figsize=(10 * pltsize, pltsize))\n",
    "for i in range(10):\n",
    "    plt.subplot(1, 10, i + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(X_train[i, :, :, :].numpy().reshape(28, 28), cmap = \"gray_r\")\n",
    "    plt.title('Class: ' + str(y_train[i].item()))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46902b7-efda-43fa-9f8c-7bfef7755cc4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0561a008-b4f4-4662-bd75-79461202a65f",
   "metadata": {},
   "source": [
    "## Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1d0106-7788-451e-9db9-b43626d54232",
   "metadata": {},
   "source": [
    "### 모델 설계"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c6250fe-5372-4848-a0a8-7bcb6a0d84d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "        self.dropout_prob = 0.5\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = self.fc1(x)\n",
    "        x = F.sigmoid(x)\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob)\n",
    "        x = self.fc2(x)\n",
    "        x = F.sigmoid(x)\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob)\n",
    "        x = self.fc3(x)\n",
    "        x = F.log_softmax(x, dim = 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39df9621-5e32-4c0e-83d1-12b7f903cd31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n",
      "Train Epoch: 1 [0/60000 (0%)]\tTrain Loss: 2.426050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adam/opt/anaconda3/lib/python3.8/site-packages/torch/nn/functional.py:1805: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [6400/60000 (11%)]\tTrain Loss: 2.328610\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tTrain Loss: 2.306461\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tTrain Loss: 2.274695\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tTrain Loss: 2.357585\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tTrain Loss: 2.328877\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tTrain Loss: 2.349853\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tTrain Loss: 2.273973\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tTrain Loss: 2.260295\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tTrain Loss: 2.345739\n",
      "\n",
      "[EPOCH: 1], \tTest Loss: 0.0713, \tTest Accuracy: 12.18 % \n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tTrain Loss: 2.265782\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tTrain Loss: 2.339506\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tTrain Loss: 2.262650\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tTrain Loss: 2.330726\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tTrain Loss: 2.297290\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tTrain Loss: 2.232897\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tTrain Loss: 2.290023\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tTrain Loss: 2.102069\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tTrain Loss: 2.134834\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tTrain Loss: 2.083320\n",
      "\n",
      "[EPOCH: 2], \tTest Loss: 0.0622, \tTest Accuracy: 33.90 % \n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tTrain Loss: 2.008444\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tTrain Loss: 2.014210\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tTrain Loss: 2.001516\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tTrain Loss: 1.776831\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tTrain Loss: 1.713905\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tTrain Loss: 1.683160\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tTrain Loss: 1.646054\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tTrain Loss: 1.504577\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tTrain Loss: 1.196145\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tTrain Loss: 1.773868\n",
      "\n",
      "[EPOCH: 3], \tTest Loss: 0.0391, \tTest Accuracy: 60.41 % \n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tTrain Loss: 1.629417\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tTrain Loss: 1.595873\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tTrain Loss: 1.500468\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tTrain Loss: 1.275905\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tTrain Loss: 1.283223\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tTrain Loss: 1.132084\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tTrain Loss: 1.232978\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tTrain Loss: 1.158128\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tTrain Loss: 1.169152\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tTrain Loss: 0.970220\n",
      "\n",
      "[EPOCH: 4], \tTest Loss: 0.0280, \tTest Accuracy: 71.21 % \n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tTrain Loss: 1.219005\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tTrain Loss: 1.272850\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tTrain Loss: 1.011117\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tTrain Loss: 0.969156\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tTrain Loss: 1.179886\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tTrain Loss: 1.023459\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tTrain Loss: 0.795861\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tTrain Loss: 1.266914\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tTrain Loss: 1.043187\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tTrain Loss: 0.864847\n",
      "\n",
      "[EPOCH: 5], \tTest Loss: 0.0237, \tTest Accuracy: 75.85 % \n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tTrain Loss: 0.898121\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tTrain Loss: 0.645375\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tTrain Loss: 1.073805\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tTrain Loss: 1.003803\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tTrain Loss: 0.844410\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tTrain Loss: 0.660867\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tTrain Loss: 1.410566\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tTrain Loss: 0.782981\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tTrain Loss: 0.873253\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tTrain Loss: 0.844249\n",
      "\n",
      "[EPOCH: 6], \tTest Loss: 0.0205, \tTest Accuracy: 80.27 % \n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tTrain Loss: 0.835321\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tTrain Loss: 0.840825\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tTrain Loss: 0.899452\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tTrain Loss: 1.172268\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tTrain Loss: 0.613468\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tTrain Loss: 0.833060\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tTrain Loss: 0.484597\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tTrain Loss: 0.763231\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tTrain Loss: 0.727098\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tTrain Loss: 1.036727\n",
      "\n",
      "[EPOCH: 7], \tTest Loss: 0.0179, \tTest Accuracy: 83.10 % \n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tTrain Loss: 0.945110\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tTrain Loss: 0.375716\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tTrain Loss: 1.010692\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tTrain Loss: 0.605725\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tTrain Loss: 1.044559\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tTrain Loss: 0.630428\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tTrain Loss: 0.933726\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tTrain Loss: 0.603780\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tTrain Loss: 0.651263\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tTrain Loss: 0.472424\n",
      "\n",
      "[EPOCH: 8], \tTest Loss: 0.0162, \tTest Accuracy: 84.90 % \n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tTrain Loss: 0.761607\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tTrain Loss: 0.799231\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tTrain Loss: 0.904680\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tTrain Loss: 0.473590\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tTrain Loss: 0.779088\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tTrain Loss: 0.483803\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tTrain Loss: 0.391402\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tTrain Loss: 0.846472\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tTrain Loss: 1.091629\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tTrain Loss: 0.456327\n",
      "\n",
      "[EPOCH: 9], \tTest Loss: 0.0148, \tTest Accuracy: 86.06 % \n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tTrain Loss: 0.918912\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tTrain Loss: 0.928971\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tTrain Loss: 0.645144\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tTrain Loss: 0.437004\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tTrain Loss: 0.462252\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tTrain Loss: 0.495836\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tTrain Loss: 0.479887\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tTrain Loss: 1.046696\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tTrain Loss: 0.615206\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tTrain Loss: 0.464082\n",
      "\n",
      "[EPOCH: 10], \tTest Loss: 0.0139, \tTest Accuracy: 87.07 % \n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' Optimizer, Objective Function 설정하기 '''\n",
    "model = Net().to(DEVICE)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(model)\n",
    "\n",
    "''' MLP 모델 학습을 진행하며 학습 데이터에 대한 모델 성능을 확인하는 함수 정의 '''\n",
    "def train(model, train_loader, optimizer, log_interval):\n",
    "    model.train()\n",
    "    for batch_idx, (image, label) in enumerate(train_loader):\n",
    "        image = image.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(image)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tTrain Loss: {:.6f}\".format(\n",
    "                epoch, batch_idx * len(image), \n",
    "                len(train_loader.dataset), 100. * batch_idx / len(train_loader), \n",
    "                loss.item()))\n",
    "            \n",
    "''' 학습되는 과정 속에서 검증 데이터에 대한 모델 성능을 확인하는 함수 정의 '''\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, label in test_loader:\n",
    "            image = image.to(DEVICE)\n",
    "            label = label.to(DEVICE)\n",
    "            output = model(image)\n",
    "            test_loss += criterion(output, label).item()\n",
    "            prediction = output.max(1, keepdim = True)[1]\n",
    "            correct += prediction.eq(label.view_as(prediction)).sum().item()\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, test_accuracy  \n",
    "\n",
    "''' MLP 학습 실행하며 Train, Test set의 Loss 및 Test set Accuracy 확인하기 '''\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(model, train_loader, optimizer, log_interval = 200)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    print(\"\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} % \\n\".format(\n",
    "        epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e581889-3ab6-46f4-966d-2c5508e255c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4df45a4-778b-4425-bad8-e9d3b537fa0e",
   "metadata": {},
   "source": [
    "## Dropout + ReLU 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "97116ac4-3b47-4074-b1df-1084cc6e9ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "        self.dropout_prob = 0.5\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob)\n",
    "        x = self.fc3(x)\n",
    "        x = F.log_softmax(x, dim = 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba1b42be-c919-4e21-9b3b-2a2c08f18b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n",
      "Train Epoch: 1 [0/60000 (0%)]\tTrain Loss: 2.303803\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tTrain Loss: 2.044045\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tTrain Loss: 0.926277\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tTrain Loss: 1.173091\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tTrain Loss: 0.641563\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tTrain Loss: 0.836639\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tTrain Loss: 0.523656\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tTrain Loss: 0.401377\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tTrain Loss: 0.375473\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tTrain Loss: 0.533670\n",
      "\n",
      "[EPOCH: 1], \tTest Loss: 0.0100, \tTest Accuracy: 91.04 % \n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tTrain Loss: 0.456933\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tTrain Loss: 0.305147\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tTrain Loss: 0.313585\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tTrain Loss: 0.194079\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tTrain Loss: 0.389138\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tTrain Loss: 0.328343\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tTrain Loss: 0.169831\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tTrain Loss: 0.313343\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tTrain Loss: 0.246108\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tTrain Loss: 0.206348\n",
      "\n",
      "[EPOCH: 2], \tTest Loss: 0.0069, \tTest Accuracy: 93.64 % \n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tTrain Loss: 0.468015\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tTrain Loss: 0.319489\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tTrain Loss: 0.205709\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tTrain Loss: 0.163577\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tTrain Loss: 0.139949\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tTrain Loss: 0.228519\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tTrain Loss: 0.243396\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tTrain Loss: 0.235951\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tTrain Loss: 0.116234\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tTrain Loss: 0.096054\n",
      "\n",
      "[EPOCH: 3], \tTest Loss: 0.0055, \tTest Accuracy: 94.75 % \n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tTrain Loss: 0.156945\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tTrain Loss: 0.086275\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tTrain Loss: 0.307841\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tTrain Loss: 0.238401\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tTrain Loss: 0.077299\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tTrain Loss: 0.543458\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tTrain Loss: 0.426077\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tTrain Loss: 0.300849\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tTrain Loss: 0.226229\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tTrain Loss: 0.232219\n",
      "\n",
      "[EPOCH: 4], \tTest Loss: 0.0046, \tTest Accuracy: 95.52 % \n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tTrain Loss: 0.312296\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tTrain Loss: 0.242673\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tTrain Loss: 0.392063\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tTrain Loss: 0.060423\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tTrain Loss: 0.199582\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tTrain Loss: 0.153133\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tTrain Loss: 0.090365\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tTrain Loss: 0.208260\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tTrain Loss: 0.138894\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tTrain Loss: 0.135291\n",
      "\n",
      "[EPOCH: 5], \tTest Loss: 0.0040, \tTest Accuracy: 96.04 % \n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tTrain Loss: 0.237749\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tTrain Loss: 0.167339\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tTrain Loss: 0.057827\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tTrain Loss: 0.146547\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tTrain Loss: 0.059410\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tTrain Loss: 0.049746\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tTrain Loss: 0.104288\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tTrain Loss: 0.304822\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tTrain Loss: 0.359897\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tTrain Loss: 0.122632\n",
      "\n",
      "[EPOCH: 6], \tTest Loss: 0.0036, \tTest Accuracy: 96.25 % \n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tTrain Loss: 0.107101\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tTrain Loss: 0.150192\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tTrain Loss: 0.163914\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tTrain Loss: 0.113495\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tTrain Loss: 0.137272\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tTrain Loss: 0.201739\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tTrain Loss: 0.106377\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tTrain Loss: 0.061688\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tTrain Loss: 0.153860\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tTrain Loss: 0.020210\n",
      "\n",
      "[EPOCH: 7], \tTest Loss: 0.0031, \tTest Accuracy: 96.90 % \n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tTrain Loss: 0.069622\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tTrain Loss: 0.162143\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tTrain Loss: 0.191687\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tTrain Loss: 0.210062\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tTrain Loss: 0.053123\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tTrain Loss: 0.089504\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tTrain Loss: 0.275118\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tTrain Loss: 0.065420\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tTrain Loss: 0.342825\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tTrain Loss: 0.067163\n",
      "\n",
      "[EPOCH: 8], \tTest Loss: 0.0030, \tTest Accuracy: 96.94 % \n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tTrain Loss: 0.035195\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tTrain Loss: 0.111971\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tTrain Loss: 0.275686\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tTrain Loss: 0.044472\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tTrain Loss: 0.205157\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tTrain Loss: 0.084324\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tTrain Loss: 0.139502\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tTrain Loss: 0.353327\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tTrain Loss: 0.080251\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tTrain Loss: 0.067205\n",
      "\n",
      "[EPOCH: 9], \tTest Loss: 0.0028, \tTest Accuracy: 97.15 % \n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tTrain Loss: 0.145868\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tTrain Loss: 0.205886\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tTrain Loss: 0.261423\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tTrain Loss: 0.039390\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tTrain Loss: 0.107762\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tTrain Loss: 0.224818\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tTrain Loss: 0.100606\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tTrain Loss: 0.077184\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tTrain Loss: 0.142246\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tTrain Loss: 0.232236\n",
      "\n",
      "[EPOCH: 10], \tTest Loss: 0.0026, \tTest Accuracy: 97.40 % \n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' Optimizer, Objective Function 설정하기 '''\n",
    "model = Net().to(DEVICE)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(model)\n",
    "\n",
    "''' MLP 모델 학습을 진행하며 학습 데이터에 대한 모델 성능을 확인하는 함수 정의 '''\n",
    "def train(model, train_loader, optimizer, log_interval):\n",
    "    model.train()\n",
    "    for batch_idx, (image, label) in enumerate(train_loader):\n",
    "        image = image.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(image)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tTrain Loss: {:.6f}\".format(\n",
    "                epoch, batch_idx * len(image), \n",
    "                len(train_loader.dataset), 100. * batch_idx / len(train_loader), \n",
    "                loss.item()))\n",
    "            \n",
    "''' 학습되는 과정 속에서 검증 데이터에 대한 모델 성능을 확인하는 함수 정의 '''\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, label in test_loader:\n",
    "            image = image.to(DEVICE)\n",
    "            label = label.to(DEVICE)\n",
    "            output = model(image)\n",
    "            test_loss += criterion(output, label).item()\n",
    "            prediction = output.max(1, keepdim = True)[1]\n",
    "            correct += prediction.eq(label.view_as(prediction)).sum().item()\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, test_accuracy  \n",
    "\n",
    "''' MLP 학습 실행하며 Train, Test set의 Loss 및 Test set Accuracy 확인하기 '''\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(model, train_loader, optimizer, log_interval = 200)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    print(\"\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} % \\n\".format(\n",
    "        epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6f46be-39fb-4310-be20-dacbc20a69d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd44cf54-8301-469f-a41b-445a8a79e79b",
   "metadata": {},
   "source": [
    "## Dropout + ReLU + Batch Normalization 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf4698ee-138d-4023-bc1c-d2dd954159b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Multi Layer Perceptron (MLP) 모델 설계하기 '''\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, 10)\n",
    "        self.dropout_prob = 0.5\n",
    "        self.batch_norm1 = nn.BatchNorm1d(512)\n",
    "        self.batch_norm2 = nn.BatchNorm1d(256)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = self.fc1(x)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob)\n",
    "        x = self.fc2(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, training = self.training, p = self.dropout_prob)\n",
    "        x = self.fc3(x)\n",
    "        x = F.log_softmax(x, dim = 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c8937bc-e6b5-4941-9d1d-a98ccf740918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
      "  (batch_norm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batch_norm2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "Train Epoch: 1 [0/60000 (0%)]\tTrain Loss: 2.431203\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tTrain Loss: 0.400178\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tTrain Loss: 0.205539\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tTrain Loss: 0.147435\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tTrain Loss: 0.314690\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tTrain Loss: 0.614389\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tTrain Loss: 0.159236\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tTrain Loss: 0.231939\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tTrain Loss: 0.201809\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tTrain Loss: 0.385036\n",
      "\n",
      "[EPOCH: 1], \tTest Loss: 0.0049, \tTest Accuracy: 95.31 % \n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tTrain Loss: 0.217888\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tTrain Loss: 0.683931\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tTrain Loss: 0.323314\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tTrain Loss: 0.233803\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tTrain Loss: 0.203566\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tTrain Loss: 0.105223\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tTrain Loss: 0.238796\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tTrain Loss: 0.317492\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tTrain Loss: 0.382908\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tTrain Loss: 0.226305\n",
      "\n",
      "[EPOCH: 2], \tTest Loss: 0.0036, \tTest Accuracy: 96.68 % \n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tTrain Loss: 0.205655\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tTrain Loss: 0.133596\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tTrain Loss: 0.069154\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tTrain Loss: 0.518079\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tTrain Loss: 0.154404\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tTrain Loss: 0.076439\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tTrain Loss: 0.061501\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tTrain Loss: 0.208051\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tTrain Loss: 0.118459\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tTrain Loss: 0.052160\n",
      "\n",
      "[EPOCH: 3], \tTest Loss: 0.0030, \tTest Accuracy: 97.16 % \n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tTrain Loss: 0.127403\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tTrain Loss: 0.197228\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tTrain Loss: 0.332289\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tTrain Loss: 0.374807\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tTrain Loss: 0.143032\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tTrain Loss: 0.477562\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tTrain Loss: 0.112873\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tTrain Loss: 0.273531\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tTrain Loss: 0.100337\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tTrain Loss: 0.068837\n",
      "\n",
      "[EPOCH: 4], \tTest Loss: 0.0028, \tTest Accuracy: 97.21 % \n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tTrain Loss: 0.174670\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tTrain Loss: 0.169988\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tTrain Loss: 0.167093\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tTrain Loss: 0.031355\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tTrain Loss: 0.154473\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tTrain Loss: 0.125360\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tTrain Loss: 0.140159\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tTrain Loss: 0.233199\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tTrain Loss: 0.354374\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tTrain Loss: 0.150869\n",
      "\n",
      "[EPOCH: 5], \tTest Loss: 0.0025, \tTest Accuracy: 97.42 % \n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tTrain Loss: 0.040726\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tTrain Loss: 0.164931\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tTrain Loss: 0.395718\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tTrain Loss: 0.337625\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tTrain Loss: 0.114510\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tTrain Loss: 0.182675\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tTrain Loss: 0.201723\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tTrain Loss: 0.185627\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tTrain Loss: 0.107738\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tTrain Loss: 0.090439\n",
      "\n",
      "[EPOCH: 6], \tTest Loss: 0.0024, \tTest Accuracy: 97.77 % \n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tTrain Loss: 0.070474\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tTrain Loss: 0.072573\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tTrain Loss: 0.064088\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tTrain Loss: 0.094348\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tTrain Loss: 0.042420\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tTrain Loss: 0.140958\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tTrain Loss: 0.198424\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tTrain Loss: 0.028768\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tTrain Loss: 0.130669\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tTrain Loss: 0.211008\n",
      "\n",
      "[EPOCH: 7], \tTest Loss: 0.0023, \tTest Accuracy: 97.72 % \n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tTrain Loss: 0.152541\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tTrain Loss: 0.049419\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tTrain Loss: 0.095354\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tTrain Loss: 0.217243\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tTrain Loss: 0.140805\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tTrain Loss: 0.064609\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tTrain Loss: 0.163114\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tTrain Loss: 0.035632\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tTrain Loss: 0.129165\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tTrain Loss: 0.093000\n",
      "\n",
      "[EPOCH: 8], \tTest Loss: 0.0022, \tTest Accuracy: 97.82 % \n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tTrain Loss: 0.049374\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tTrain Loss: 0.167357\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tTrain Loss: 0.227300\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tTrain Loss: 0.180942\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tTrain Loss: 0.210513\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tTrain Loss: 0.034509\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tTrain Loss: 0.039384\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tTrain Loss: 0.132938\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tTrain Loss: 0.153375\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tTrain Loss: 0.069888\n",
      "\n",
      "[EPOCH: 9], \tTest Loss: 0.0021, \tTest Accuracy: 98.00 % \n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tTrain Loss: 0.164972\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tTrain Loss: 0.022488\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tTrain Loss: 0.112960\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tTrain Loss: 0.046804\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tTrain Loss: 0.279779\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tTrain Loss: 0.016358\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tTrain Loss: 0.234879\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tTrain Loss: 0.028357\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tTrain Loss: 0.146231\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tTrain Loss: 0.013667\n",
      "\n",
      "[EPOCH: 10], \tTest Loss: 0.0020, \tTest Accuracy: 98.08 % \n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' Optimizer, Objective Function 설정하기 '''\n",
    "model = Net().to(DEVICE)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(model)\n",
    "\n",
    "''' MLP 모델 학습을 진행하며 학습 데이터에 대한 모델 성능을 확인하는 함수 정의 '''\n",
    "def train(model, train_loader, optimizer, log_interval):\n",
    "    model.train()\n",
    "    for batch_idx, (image, label) in enumerate(train_loader):\n",
    "        image = image.to(DEVICE)\n",
    "        label = label.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(image)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tTrain Loss: {:.6f}\".format(\n",
    "                epoch, batch_idx * len(image), \n",
    "                len(train_loader.dataset), 100. * batch_idx / len(train_loader), \n",
    "                loss.item()))\n",
    "            \n",
    "''' 학습되는 과정 속에서 검증 데이터에 대한 모델 성능을 확인하는 함수 정의 '''\n",
    "def evaluate(model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for image, label in test_loader:\n",
    "            image = image.to(DEVICE)\n",
    "            label = label.to(DEVICE)\n",
    "            output = model(image)\n",
    "            test_loss += criterion(output, label).item()\n",
    "            prediction = output.max(1, keepdim = True)[1]\n",
    "            correct += prediction.eq(label.view_as(prediction)).sum().item()\n",
    "    \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    test_accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, test_accuracy  \n",
    "\n",
    "''' MLP 학습 실행하며 Train, Test set의 Loss 및 Test set Accuracy 확인하기 '''\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(model, train_loader, optimizer, log_interval = 200)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    print(\"\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} % \\n\".format(\n",
    "        epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597145ed-9abf-4ed1-8b7f-cee96457a185",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d27d6c1a-1285-4c77-851b-38a0e7e70f6b",
   "metadata": {},
   "source": [
    "## 사람의 손글씨 데이터인 MNIST를 이용해 Multi Layer Perceptron 설계할 때 Dropout + ReLU + Batch Normalization + He Uniform Initialization 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef4893a7-d307-465d-8da2-646c8e1d8500",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
      "  (batch_norm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batch_norm2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "Train Epoch: 1 [0/60000 (0%)]\tTrain Loss: 3.195416\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tTrain Loss: 0.591121\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tTrain Loss: 0.579755\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tTrain Loss: 0.858828\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tTrain Loss: 0.551197\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tTrain Loss: 0.670591\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tTrain Loss: 0.512658\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tTrain Loss: 0.650460\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tTrain Loss: 0.472974\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tTrain Loss: 0.317582\n",
      "\n",
      "[EPOCH: 1], \tTest Loss: 0.0068, \tTest Accuracy: 93.52 % \n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tTrain Loss: 0.242920\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tTrain Loss: 0.479070\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tTrain Loss: 0.508630\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tTrain Loss: 0.192070\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tTrain Loss: 0.725676\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tTrain Loss: 0.305349\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tTrain Loss: 0.304257\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tTrain Loss: 0.382915\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tTrain Loss: 0.913349\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tTrain Loss: 0.390136\n",
      "\n",
      "[EPOCH: 2], \tTest Loss: 0.0054, \tTest Accuracy: 94.76 % \n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tTrain Loss: 0.353452\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tTrain Loss: 0.349885\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tTrain Loss: 0.135657\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tTrain Loss: 0.345665\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tTrain Loss: 0.300990\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tTrain Loss: 0.293550\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tTrain Loss: 0.514715\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tTrain Loss: 0.190446\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tTrain Loss: 0.123769\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tTrain Loss: 0.330048\n",
      "\n",
      "[EPOCH: 3], \tTest Loss: 0.0045, \tTest Accuracy: 95.57 % \n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tTrain Loss: 0.358800\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tTrain Loss: 0.365421\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tTrain Loss: 0.141492\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tTrain Loss: 0.201874\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tTrain Loss: 0.264748\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tTrain Loss: 0.236919\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tTrain Loss: 0.408654\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tTrain Loss: 0.357529\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tTrain Loss: 0.514565\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tTrain Loss: 0.160602\n",
      "\n",
      "[EPOCH: 4], \tTest Loss: 0.0040, \tTest Accuracy: 96.11 % \n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tTrain Loss: 0.163728\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tTrain Loss: 0.174389\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tTrain Loss: 0.366322\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tTrain Loss: 0.222447\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tTrain Loss: 0.349744\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tTrain Loss: 0.142497\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tTrain Loss: 0.253823\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tTrain Loss: 0.235042\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tTrain Loss: 0.268046\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tTrain Loss: 0.071248\n",
      "\n",
      "[EPOCH: 5], \tTest Loss: 0.0037, \tTest Accuracy: 96.40 % \n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tTrain Loss: 0.262952\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tTrain Loss: 0.085853\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tTrain Loss: 0.283294\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tTrain Loss: 0.309472\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tTrain Loss: 0.089765\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tTrain Loss: 0.523939\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tTrain Loss: 0.276865\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tTrain Loss: 0.458289\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tTrain Loss: 0.264890\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tTrain Loss: 0.201213\n",
      "\n",
      "[EPOCH: 6], \tTest Loss: 0.0034, \tTest Accuracy: 96.87 % \n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tTrain Loss: 0.276964\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tTrain Loss: 0.123380\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tTrain Loss: 0.535059\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tTrain Loss: 0.246102\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tTrain Loss: 0.139066\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tTrain Loss: 0.121300\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tTrain Loss: 0.084826\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tTrain Loss: 0.127315\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tTrain Loss: 0.151467\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tTrain Loss: 0.151354\n",
      "\n",
      "[EPOCH: 7], \tTest Loss: 0.0032, \tTest Accuracy: 96.77 % \n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tTrain Loss: 0.327321\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tTrain Loss: 0.359838\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tTrain Loss: 0.066135\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tTrain Loss: 0.099205\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tTrain Loss: 0.066212\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tTrain Loss: 0.477962\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tTrain Loss: 0.648165\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tTrain Loss: 0.205280\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tTrain Loss: 0.113383\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tTrain Loss: 0.247976\n",
      "\n",
      "[EPOCH: 8], \tTest Loss: 0.0031, \tTest Accuracy: 96.83 % \n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tTrain Loss: 0.330729\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tTrain Loss: 0.231401\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tTrain Loss: 0.087232\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tTrain Loss: 0.030022\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tTrain Loss: 0.149084\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tTrain Loss: 0.094590\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tTrain Loss: 0.473994\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tTrain Loss: 0.118542\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tTrain Loss: 0.232666\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tTrain Loss: 0.099488\n",
      "\n",
      "[EPOCH: 9], \tTest Loss: 0.0029, \tTest Accuracy: 97.08 % \n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tTrain Loss: 0.259243\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tTrain Loss: 0.226737\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tTrain Loss: 0.123401\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tTrain Loss: 0.325431\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tTrain Loss: 0.045567\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tTrain Loss: 0.057215\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tTrain Loss: 0.018119\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tTrain Loss: 0.264866\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tTrain Loss: 0.119318\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tTrain Loss: 0.137515\n",
      "\n",
      "[EPOCH: 10], \tTest Loss: 0.0028, \tTest Accuracy: 97.25 % \n",
      "\n"
     ]
    }
   ],
   "source": [
    "''' Optimizer, Objective Function 설정하기 '''\n",
    "import torch.nn.init as init\n",
    "def weight_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        init.kaiming_uniform_(m.weight.data)\n",
    "\n",
    "model = Net().to(DEVICE)\n",
    "model.apply(weight_init)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(model)\n",
    "\n",
    "''' MLP 학습 실행하며 Train, Test set의 Loss 및 Test set Accuracy 확인하기 '''\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(model, train_loader, optimizer, log_interval = 200)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    print(\"\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} % \\n\".format(\n",
    "        epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7fcdb7-7cab-489a-a4d7-cb6cbbc25598",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e6c82a1d-34b2-4c7a-a7e6-f38647964fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=256, bias=True)\n",
      "  (fc3): Linear(in_features=256, out_features=10, bias=True)\n",
      "  (batch_norm1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (batch_norm2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "Train Epoch: 1 [0/60000 (0%)]\tTrain Loss: 3.091649\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tTrain Loss: 0.169459\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tTrain Loss: 0.342087\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tTrain Loss: 0.588417\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tTrain Loss: 0.599481\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tTrain Loss: 0.383133\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tTrain Loss: 0.187853\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tTrain Loss: 0.475380\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tTrain Loss: 0.330638\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tTrain Loss: 0.260193\n",
      "\n",
      "[EPOCH: 1], \tTest Loss: 0.0041, \tTest Accuracy: 95.97 % \n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tTrain Loss: 0.559042\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tTrain Loss: 0.350580\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tTrain Loss: 0.172170\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tTrain Loss: 0.167987\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tTrain Loss: 0.198966\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tTrain Loss: 0.197381\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tTrain Loss: 0.139494\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tTrain Loss: 0.267552\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tTrain Loss: 0.054039\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tTrain Loss: 0.035694\n",
      "\n",
      "[EPOCH: 2], \tTest Loss: 0.0032, \tTest Accuracy: 96.70 % \n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tTrain Loss: 0.213726\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tTrain Loss: 0.387418\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tTrain Loss: 0.126004\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tTrain Loss: 0.221575\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tTrain Loss: 0.394136\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tTrain Loss: 0.178659\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tTrain Loss: 0.378220\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tTrain Loss: 0.423538\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tTrain Loss: 0.247762\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tTrain Loss: 0.550266\n",
      "\n",
      "[EPOCH: 3], \tTest Loss: 0.0029, \tTest Accuracy: 97.18 % \n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tTrain Loss: 0.063708\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tTrain Loss: 0.382530\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tTrain Loss: 0.146604\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tTrain Loss: 0.272654\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tTrain Loss: 0.331935\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tTrain Loss: 0.101165\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tTrain Loss: 0.265309\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tTrain Loss: 0.296627\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tTrain Loss: 0.242335\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tTrain Loss: 0.038072\n",
      "\n",
      "[EPOCH: 4], \tTest Loss: 0.0028, \tTest Accuracy: 97.27 % \n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tTrain Loss: 0.268263\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tTrain Loss: 0.245374\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tTrain Loss: 0.083119\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tTrain Loss: 0.432383\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tTrain Loss: 0.208037\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tTrain Loss: 0.052054\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tTrain Loss: 0.217691\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tTrain Loss: 0.185358\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tTrain Loss: 0.026150\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tTrain Loss: 0.179087\n",
      "\n",
      "[EPOCH: 5], \tTest Loss: 0.0025, \tTest Accuracy: 97.60 % \n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tTrain Loss: 0.251455\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tTrain Loss: 0.463863\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tTrain Loss: 0.053658\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tTrain Loss: 0.038309\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tTrain Loss: 0.340784\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tTrain Loss: 0.153403\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tTrain Loss: 0.332626\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tTrain Loss: 0.405558\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tTrain Loss: 0.083836\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tTrain Loss: 0.167335\n",
      "\n",
      "[EPOCH: 6], \tTest Loss: 0.0025, \tTest Accuracy: 97.52 % \n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tTrain Loss: 0.179948\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tTrain Loss: 0.097232\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tTrain Loss: 0.319602\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tTrain Loss: 0.118180\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tTrain Loss: 0.426741\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tTrain Loss: 0.784070\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tTrain Loss: 0.164753\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tTrain Loss: 0.094267\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tTrain Loss: 0.246923\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tTrain Loss: 0.120522\n",
      "\n",
      "[EPOCH: 7], \tTest Loss: 0.0023, \tTest Accuracy: 97.88 % \n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tTrain Loss: 0.145145\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tTrain Loss: 0.108236\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tTrain Loss: 0.274999\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tTrain Loss: 0.448393\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tTrain Loss: 0.369174\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tTrain Loss: 0.118347\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tTrain Loss: 0.045290\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tTrain Loss: 0.024720\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tTrain Loss: 0.079669\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tTrain Loss: 0.333982\n",
      "\n",
      "[EPOCH: 8], \tTest Loss: 0.0024, \tTest Accuracy: 97.89 % \n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tTrain Loss: 0.061313\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tTrain Loss: 0.071202\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tTrain Loss: 0.092407\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tTrain Loss: 0.152005\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tTrain Loss: 0.265051\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tTrain Loss: 0.114897\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tTrain Loss: 0.043583\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tTrain Loss: 0.057170\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tTrain Loss: 0.045996\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tTrain Loss: 0.172932\n",
      "\n",
      "[EPOCH: 9], \tTest Loss: 0.0021, \tTest Accuracy: 97.89 % \n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tTrain Loss: 0.072833\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tTrain Loss: 0.312376\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tTrain Loss: 0.024109\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tTrain Loss: 0.145188\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tTrain Loss: 0.095985\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tTrain Loss: 0.373030\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tTrain Loss: 0.141502\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tTrain Loss: 0.352224\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tTrain Loss: 0.151886\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tTrain Loss: 0.039940\n",
      "\n",
      "[EPOCH: 10], \tTest Loss: 0.0022, \tTest Accuracy: 97.87 % \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.init as init\n",
    "def weight_init(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        init.kaiming_uniform_(m.weight.data)\n",
    "\n",
    "model = Net().to(DEVICE)\n",
    "model.apply(weight_init)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(model)\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    train(model, train_loader, optimizer, log_interval = 200)\n",
    "    test_loss, test_accuracy = evaluate(model, test_loader)\n",
    "    print(\"\\n[EPOCH: {}], \\tTest Loss: {:.4f}, \\tTest Accuracy: {:.2f} % \\n\".format(\n",
    "        epoch, test_loss, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80605f3e-0e39-4126-b572-24fb3c84c692",
   "metadata": {},
   "source": [
    "### 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a7852ec2-8398-42ac-8417-6f2888479aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 결과 : 7\n",
      "(1, 28, 28)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fe166360160>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_iterator = iter(test_loader)\n",
    "images = next(batch_iterator)\n",
    "\n",
    "model.eval()  # 신경망을 추론 모드로 전환\n",
    "\n",
    "output = model(images[0])  # 데이터를 입력하고 출력을 계산\n",
    "result = output.detach().numpy()\n",
    "print(\"예측 결과 : {}\".format(np.argmax(result[0])))\n",
    "\n",
    "image = images[0].detach().numpy()\n",
    "plt.imshow(image[0].reshape(28, 28), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "574e35b6-52ad-4b10-af7a-96fa87c0998c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "mnist = fetch_openml('mnist_784', version=1, cache=True)\n",
    "\n",
    "mnist.target = mnist.target.astype(np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ca8a0b4a-1105-4378-9bc7-50617b90ef16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측 결과 : 9\n",
      "이 이미지 데이터의 정답 레이블은 9입니다\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "X = mnist.data/255  # 0-255값을 [0,1] 구간으로 정규화\n",
    "y = mnist.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X.values, y.values, test_size=1/7, random_state=0)\n",
    "\n",
    "X_train = torch.Tensor(X_train)\n",
    "X_test = torch.Tensor(X_test)\n",
    "y_train = torch.LongTensor(y_train)\n",
    "y_test = torch.LongTensor(y_test)\n",
    "\n",
    "ds_train = TensorDataset(X_train, y_train)\n",
    "ds_test = TensorDataset(X_test, y_test)\n",
    "\n",
    "loader_train = DataLoader(ds_train, batch_size=64, shuffle=True)\n",
    "loader_test = DataLoader(ds_test, batch_size=64, shuffle=False)\n",
    "\n",
    "index = 42\n",
    "\n",
    "model.eval()  # 신경망을 추론 모드로 전환\n",
    "data = X_test[index]\n",
    "\n",
    "output = model(data)  # 데이터를 입력하고 출력을 계산\n",
    "predicted = output[0].detach().numpy()\n",
    "print(\"예측 결과 : {}\".format(np.argmax(predicted)))\n",
    "\n",
    "X_test_show = (X_test[index]).numpy()\n",
    "plt.imshow(X_test_show.reshape(28, 28), cmap='gray')\n",
    "print(\"이 이미지 데이터의 정답 레이블은 {:.0f}입니다\".format(y_test[index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e106b9c-a00e-44d5-b9e2-4599d12699e4",
   "metadata": {},
   "source": [
    "## 선형 회귀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e7a3a3b-afe2-4cd5-a1b9-d81dd0ef08b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c5332c-423f-447e-830a-08ec96f13616",
   "metadata": {},
   "source": [
    "### 단순 선형 회귀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a553127c-0484-4a12-9135-34c0f1bd0cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[0.7695]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.5948], requires_grad=True)]\n",
      "Epoch    0/2000 Cost: 10.347617\n",
      "Epoch  100/2000 Cost: 0.000178\n",
      "Epoch  200/2000 Cost: 0.000110\n",
      "Epoch  300/2000 Cost: 0.000068\n",
      "Epoch  400/2000 Cost: 0.000042\n",
      "Epoch  500/2000 Cost: 0.000026\n",
      "Epoch  600/2000 Cost: 0.000016\n",
      "Epoch  700/2000 Cost: 0.000010\n",
      "Epoch  800/2000 Cost: 0.000006\n",
      "Epoch  900/2000 Cost: 0.000004\n",
      "Epoch 1000/2000 Cost: 0.000002\n",
      "Epoch 1100/2000 Cost: 0.000001\n",
      "Epoch 1200/2000 Cost: 0.000001\n",
      "Epoch 1300/2000 Cost: 0.000001\n",
      "Epoch 1400/2000 Cost: 0.000000\n",
      "Epoch 1500/2000 Cost: 0.000000\n",
      "Epoch 1600/2000 Cost: 0.000000\n",
      "Epoch 1700/2000 Cost: 0.000000\n",
      "Epoch 1800/2000 Cost: 0.000000\n",
      "Epoch 1900/2000 Cost: 0.000000\n",
      "Epoch 2000/2000 Cost: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# 데이터\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])\n",
    "\n",
    "# 단순 선형 회귀 모델 생성 \n",
    "# input_dim=1, output_dim=1.\n",
    "model = nn.Linear(1,1)\n",
    "\n",
    "# 모델의 하이퍼파라미터 출력\n",
    "print(list(model.parameters()))\n",
    "\n",
    "#최적화 함수 선택\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01) \n",
    "\n",
    "# 전체 훈련 데이터에 대해 경사 하강법을 2,000회 반복\n",
    "nb_epochs = 2000\n",
    "for epoch in range(nb_epochs+1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "\n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train) # <== 파이토치에서 제공하는 평균 제곱 오차 함수\n",
    "\n",
    "    # cost로 H(x) 개선하는 부분\n",
    "    # gradient를 0으로 초기화\n",
    "    optimizer.zero_grad()\n",
    "    # 비용 함수를 미분하여 gradient 계산\n",
    "    cost.backward() # backward 연산\n",
    "    # W와 b를 업데이트\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "    # 100번마다 로그 출력\n",
    "      print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "          epoch, nb_epochs, cost.item()\n",
    "      ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "710c9786-ecc4-4fcd-8eb3-877482614a05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 후 입력이 4일 때의 예측값 : tensor([[8.0003]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# 임의의 입력 4를 선언\n",
    "new_var =  torch.FloatTensor([[4.0]]) \n",
    "# 입력한 값 4에 대해서 예측값 y를 리턴받아서 pred_y에 저장\n",
    "pred_y = model(new_var) # forward 연산\n",
    "# y = 2x 이므로 입력이 4라면 y가 8에 가까운 값이 나와야 제대로 학습이 된 것\n",
    "print(\"훈련 후 입력이 4일 때의 예측값 :\", pred_y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2092418f-e566-4023-b6a7-0c6ea262a804",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[2.0002]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.0004], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "050bb213-0a86-4c9d-8e17-352fce96ade4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1beceb12-3a76-415d-a241-ac54755cbf5e",
   "metadata": {},
   "source": [
    "### 단순 선형 회귀 - 클래스로 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca32f583-1b35-4e8c-9d6c-752611fa7a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# 데이터\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d7f049d-ff0a-4d15-92df-a411d60ccf06",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55e87f3e-3103-4dd7-805e-486f08492fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegressionModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bbf411fb-f393-4d72-b54c-605904aa9115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer 설정. 경사 하강법 SGD를 사용하고 learning rate를 의미하는 lr은 0.01\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "151c7f55-3168-4c99-90bc-2e5289e79fdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/2000 Cost: 3.710179\n",
      "Epoch  100/2000 Cost: 0.117398\n",
      "Epoch  200/2000 Cost: 0.072545\n",
      "Epoch  300/2000 Cost: 0.044828\n",
      "Epoch  400/2000 Cost: 0.027701\n",
      "Epoch  500/2000 Cost: 0.017118\n",
      "Epoch  600/2000 Cost: 0.010578\n",
      "Epoch  700/2000 Cost: 0.006536\n",
      "Epoch  800/2000 Cost: 0.004039\n",
      "Epoch  900/2000 Cost: 0.002496\n",
      "Epoch 1000/2000 Cost: 0.001542\n",
      "Epoch 1100/2000 Cost: 0.000953\n",
      "Epoch 1200/2000 Cost: 0.000589\n",
      "Epoch 1300/2000 Cost: 0.000364\n",
      "Epoch 1400/2000 Cost: 0.000225\n",
      "Epoch 1500/2000 Cost: 0.000139\n",
      "Epoch 1600/2000 Cost: 0.000086\n",
      "Epoch 1700/2000 Cost: 0.000053\n",
      "Epoch 1800/2000 Cost: 0.000033\n",
      "Epoch 1900/2000 Cost: 0.000020\n",
      "Epoch 2000/2000 Cost: 0.000013\n"
     ]
    }
   ],
   "source": [
    "# 전체 훈련 데이터에 대해 경사 하강법을 2,000회 반복\n",
    "nb_epochs = 2000\n",
    "for epoch in range(nb_epochs+1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "\n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train) # <== 파이토치에서 제공하는 평균 제곱 오차 함수\n",
    "\n",
    "    # cost로 H(x) 개선하는 부분\n",
    "    # gradient를 0으로 초기화\n",
    "    optimizer.zero_grad()\n",
    "    # 비용 함수를 미분하여 gradient 계산\n",
    "    cost.backward() # backward 연산\n",
    "    # W와 b를 업데이트\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "    # 100번마다 로그 출력\n",
    "      print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "          epoch, nb_epochs, cost.item()\n",
    "      ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fddb476-0bb9-403b-9e6d-6bd0385c4396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 후 입력이 4일 때의 예측값 : tensor([[7.9929]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# 임의의 입력 4를 선언\n",
    "new_var =  torch.FloatTensor([[4.0]]) \n",
    "# 입력한 값 4에 대해서 예측값 y를 리턴받아서 pred_y에 저장\n",
    "pred_y = model(new_var) # forward 연산\n",
    "# y = 2x 이므로 입력이 4라면 y가 8에 가까운 값이 나와야 제대로 학습이 된 것\n",
    "print(\"훈련 후 입력이 4일 때의 예측값 :\", pred_y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6adc146c-ae83-4254-aefe-ab7fec06454f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[1.9959]], requires_grad=True), Parameter containing:\n",
      "tensor([0.0093], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a638f9-3184-4ee8-9f58-38fca8c15cd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f6632f1f-710b-46c1-ac28-734fc3d554e8",
   "metadata": {},
   "source": [
    "### 다중 선형 회귀 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f49d25c-3cbb-493c-85f2-5c3e62ea3ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# 데이터\n",
    "x_train = torch.FloatTensor([[73, 80, 75],\n",
    "                             [93, 88, 93],\n",
    "                             [89, 91, 90],\n",
    "                             [96, 98, 100],\n",
    "                             [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0470edbb-dd32-47bf-84f5-2f2045cfeb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultivariateLinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3, 1) # 다중 선형 회귀이므로 input_dim=3, output_dim=1.\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "51329699-8bd6-42f2-9250-fe79d70f51e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultivariateLinearRegressionModel()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a465edf-e13f-4fb8-92b5-2298ad13188e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/2000 Cost: 10995.318359\n",
      "Epoch  100/2000 Cost: 2.533191\n",
      "Epoch  200/2000 Cost: 2.409125\n",
      "Epoch  300/2000 Cost: 2.291532\n",
      "Epoch  400/2000 Cost: 2.180158\n",
      "Epoch  500/2000 Cost: 2.074660\n",
      "Epoch  600/2000 Cost: 1.974722\n",
      "Epoch  700/2000 Cost: 1.880020\n",
      "Epoch  800/2000 Cost: 1.790340\n",
      "Epoch  900/2000 Cost: 1.705363\n",
      "Epoch 1000/2000 Cost: 1.624877\n",
      "Epoch 1100/2000 Cost: 1.548606\n",
      "Epoch 1200/2000 Cost: 1.476381\n",
      "Epoch 1300/2000 Cost: 1.407951\n",
      "Epoch 1400/2000 Cost: 1.343122\n",
      "Epoch 1500/2000 Cost: 1.281718\n",
      "Epoch 1600/2000 Cost: 1.223515\n",
      "Epoch 1700/2000 Cost: 1.168411\n",
      "Epoch 1800/2000 Cost: 1.116184\n",
      "Epoch 1900/2000 Cost: 1.066732\n",
      "Epoch 2000/2000 Cost: 1.019865\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 2000\n",
    "for epoch in range(nb_epochs+1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    prediction = model(x_train)\n",
    "    # model(x_train)은 model.forward(x_train)와 동일함.\n",
    "\n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y_train) # <== 파이토치에서 제공하는 평균 제곱 오차 함수\n",
    "\n",
    "    # cost로 H(x) 개선하는 부분\n",
    "    # gradient를 0으로 초기화\n",
    "    optimizer.zero_grad()\n",
    "    # 비용 함수를 미분하여 gradient 계산\n",
    "    cost.backward()\n",
    "    # W와 b를 업데이트\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "    # 100번마다 로그 출력\n",
    "      print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "          epoch, nb_epochs, cost.item()\n",
    "      ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4fed684c-53f9-4a93-ad2c-867bec8f0e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 후 입력이 [73, 80, 74]일 때의 예측값 : tensor([[152.7107]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# 임의의 입력\n",
    "new_var =  torch.FloatTensor([[73, 80, 74]]) \n",
    "pred_y = model(new_var) # forward 연산\n",
    "print(\"훈련 후 입력이 [73, 80, 74]일 때의 예측값 :\", pred_y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cde28b65-09f7-4925-b93b-1e15c9afeef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[0.9541, 0.7449, 0.3099]], requires_grad=True), Parameter containing:\n",
      "tensor([0.5357], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e438a0-9afe-4806-9779-792b7a0b7213",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0cadaec0-8106-4442-a42c-25a6d80c031a",
   "metadata": {},
   "source": [
    "### CSV 파일을 읽어서 선형 회귀 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a519407-699f-458b-ba09-2fa9a0c41de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8accdc10-234d-4af1-8bb4-cc43dd515efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  name  score   iq  academy  game  tv\n",
      "0    A     90  140        2     1   0\n",
      "1    B     75  125        1     3   3\n",
      "2    C     77  120        1     0   4\n",
      "3    D     83  135        2     3   2\n",
      "4    E     65  105        0     4   4\n",
      "tensor([[[140.,   2.,   1.,   0.]],\n",
      "\n",
      "        [[125.,   1.,   3.,   3.]],\n",
      "\n",
      "        [[120.,   1.,   0.,   4.]],\n",
      "\n",
      "        [[135.,   2.,   3.,   2.]],\n",
      "\n",
      "        [[105.,   0.,   4.,   4.]],\n",
      "\n",
      "        [[123.,   3.,   1.,   1.]],\n",
      "\n",
      "        [[132.,   3.,   4.,   1.]],\n",
      "\n",
      "        [[115.,   1.,   1.,   3.]],\n",
      "\n",
      "        [[128.,   4.,   0.,   0.]],\n",
      "\n",
      "        [[131.,   2.,   2.,   3.]]])\n",
      "tensor([[90.],\n",
      "        [75.],\n",
      "        [77.],\n",
      "        [83.],\n",
      "        [65.],\n",
      "        [80.],\n",
      "        [83.],\n",
      "        [70.],\n",
      "        [87.],\n",
      "        [79.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\nplt.xlim(0, 11);    plt.ylim(0, 8)\\nplt.title('02_Linear_Regression_Model_Data')\\nplt.scatter(x, y)\\n\\nplt.show()\\n\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./data/score.csv')\n",
    "print(data.head())\n",
    "\n",
    "X = torch.from_numpy(data[['iq', 'academy', 'game', 'tv']].values).unsqueeze(dim=1).float()\n",
    "y = torch.from_numpy(data['score'].values).unsqueeze(dim=1).float()\n",
    "\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d9717685-7a1d-400d-adce-4d63f8858c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultivariateLinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(4, 1) # 다중 선형 회귀이므로 input_dim=4, output_dim=1.\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2cfe8512-ed87-4029-a7ac-c49d88ece0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultivariateLinearRegressionModel()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8a94116f-e6da-4653-9349-09efd0012062",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/42/fxl3_n31121f16yj69fbrb_m0000gn/T/ipykernel_18337/661213645.py:8: UserWarning: Using a target size (torch.Size([10, 1])) that is different to the input size (torch.Size([10, 1, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  cost = F.mse_loss(prediction, y) # <== 파이토치에서 제공하는 평균 제곱 오차 함수\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/2000 Cost: 8489.842773\n",
      "Epoch  100/2000 Cost: 85.621292\n",
      "Epoch  200/2000 Cost: 85.400291\n",
      "Epoch  300/2000 Cost: 85.182419\n",
      "Epoch  400/2000 Cost: 84.967674\n",
      "Epoch  500/2000 Cost: 84.756035\n",
      "Epoch  600/2000 Cost: 84.547386\n",
      "Epoch  700/2000 Cost: 84.341721\n",
      "Epoch  800/2000 Cost: 84.139023\n",
      "Epoch  900/2000 Cost: 83.939186\n",
      "Epoch 1000/2000 Cost: 83.742218\n",
      "Epoch 1100/2000 Cost: 83.548065\n",
      "Epoch 1200/2000 Cost: 83.356712\n",
      "Epoch 1300/2000 Cost: 83.168037\n",
      "Epoch 1400/2000 Cost: 82.982071\n",
      "Epoch 1500/2000 Cost: 82.798737\n",
      "Epoch 1600/2000 Cost: 82.618057\n",
      "Epoch 1700/2000 Cost: 82.439934\n",
      "Epoch 1800/2000 Cost: 82.264305\n",
      "Epoch 1900/2000 Cost: 82.091232\n",
      "Epoch 2000/2000 Cost: 81.920586\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 2000\n",
    "for epoch in range(nb_epochs+1):\n",
    "\n",
    "    # H(x) 계산\n",
    "    prediction = model(X)\n",
    "\n",
    "    # cost 계산\n",
    "    cost = F.mse_loss(prediction, y) # <== 파이토치에서 제공하는 평균 제곱 오차 함수\n",
    "\n",
    "    # cost로 H(x) 개선하는 부분\n",
    "    # gradient를 0으로 초기화\n",
    "    optimizer.zero_grad()\n",
    "    # 비용 함수를 미분하여 gradient 계산\n",
    "    cost.backward()\n",
    "    # W와 b를 업데이트\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "    # 100번마다 로그 출력\n",
    "      print('Epoch {:4d}/{} Cost: {:.6f}'.format(\n",
    "          epoch, nb_epochs, cost.item()\n",
    "      ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "723f2dfa-a0db-40a6-b8e3-0847de78c7f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 후 입력이 [140, 2, 1, 1]일 때의 예측값 : tensor([[86.9952]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# 임의의 입력\n",
    "new_var =  torch.FloatTensor([[140, 2, 1, 1]]) \n",
    "pred_y = model(new_var) # forward 연산\n",
    "print(\"훈련 후 입력이 [140, 2, 1, 1]일 때의 예측값 :\", pred_y) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c7cb15ee-a115-417a-812c-8c71dbf93eb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 0.6153, -0.0013, -0.1548,  0.5486]], requires_grad=True), Parameter containing:\n",
      "tensor([0.4640], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05aae7a-2d16-489b-b95d-5893e968666e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
